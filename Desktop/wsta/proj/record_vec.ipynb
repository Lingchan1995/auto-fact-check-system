{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import json\n",
    "import numpy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(sentence,model):\n",
    "    words=nltk.tokenize.word_tokenize(sentence)\n",
    "    words=[word.lower() for word in words if word.isalpha()]\n",
    "    vector=numpy.zeros(300)\n",
    "   \n",
    "    for word in words:\n",
    "        try:\n",
    "            temp=model[word]\n",
    "            vector=vector+temp\n",
    "        except:\n",
    "            continue\n",
    "    return vector.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\fary-\n",
      "[nltk_data]     tale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('model.txt',binary=False)\n",
    "nltk.download('punkt')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_temp_wikiSent.json','r') as f:\n",
    "    file_content=json.load(f)\n",
    "\n",
    "detail_content = file_content['75397']\n",
    "claim = detail_content['claim']\n",
    "detail_content['claim'] = transfer(claim,model)\n",
    "evidences=detail_content['evidence']\n",
    "for evidence in evidences:\n",
    "        evidence[2]=transfer(evidence[2],model)\n",
    "with open('word2vec-train.json','w') as f:\n",
    "    json.dump(detail_content, f, indent=2, separators=(',',':'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('train_temp_wikiSent.json','r') as f:\n",
    "    file_content=json.load(f)\n",
    "\n",
    "for key in file_content.keys():\n",
    "    detail_content = file_content[key]\n",
    "    claim = detail_content['claim']\n",
    "    detail_content['claim'] = transfer(claim,model)\n",
    "\n",
    "    evidences=detail_content['evidence']\n",
    "    for evidence in evidences:\n",
    "        evidence[2]=transfer(evidence[2],model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def account(evidences):\n",
    "    vector=numpy.zeros(300)\n",
    "    for evidence in evidences:\n",
    "        vector=vector+temp\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('lbsvm_train.vec','w') as t:\n",
    "    for key in file_content.keys():\n",
    "        detail_content = file_content[key]\n",
    "        claim = numpy.array(detail_content['claim'])\n",
    "        evidence=account(detail_content['evidence'])\n",
    "        temp=claim-evidence\n",
    "        \n",
    "        if(detail_content['label']=='SUPPORTS'):\n",
    "            t.write('1 ')\n",
    "        elif(detail_content['label']=='REFUTES'):\n",
    "            t.write('0 ')\n",
    "        else:\n",
    "            t.write('2 ')\n",
    "        for i in range(len(temp)):\n",
    "            t.write(str(i+1))\n",
    "            t.write(':')\n",
    "            t.write(str(temp[i]))\n",
    "            t.write(' ')\n",
    "        t.write('\\n')\n",
    "t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[]\n",
    "train_y=[]\n",
    "for key in file_content.keys():\n",
    "    detail_content = file_content[key]\n",
    "    claim = numpy.array(detail_content['claim'])\n",
    "    evidence=account(detail_content['evidence'])\n",
    "    temp=claim-evidence\n",
    "\n",
    "    train_x.append(temp)\n",
    "    if(detail_content['label']=='SUPPORTS'):\n",
    "        train_y.append(1)\n",
    "    elif(detail_content['label']=='REFUTES'):\n",
    "        train_y.append(0)\n",
    "    else:\n",
    "        train_y.append(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\python37\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC( probability=False)\n",
    "clf.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svc_model.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf,'svc_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=joblib.load('svc_model.pkl')\n",
    "test_x=train_x[0:1000]\n",
    "result=train_y[0:1000]\n",
    "test_y=clf.predict(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814\n"
     ]
    }
   ],
   "source": [
    "x=0\n",
    "for i in range(len(test_x)):\n",
    "    if(result[i]==test_y[i]):\n",
    "        x+=1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
