{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_esim.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"O5VNQ895wn7i","colab_type":"code","outputId":"3c42c4b1-a656-45f4-d9e3-037fa7c218c8","executionInfo":{"status":"ok","timestamp":1558972340282,"user_tz":-600,"elapsed":32672,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","··········\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HeFunbGMwuK0","colab_type":"code","outputId":"4819513a-4a25-44ff-eaec-11a72c6d22e1","executionInfo":{"status":"ok","timestamp":1558972378828,"user_tz":-600,"elapsed":4624,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":2,"outputs":[{"output_type":"stream","text":["fuse: mountpoint is not empty\n","fuse: if you are sure this is safe, use the 'nonempty' mount option\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9rl1kXGh_dhV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"35ac7bfd-c86b-4a35-d357-cc63748754a5","executionInfo":{"status":"ok","timestamp":1558972378832,"user_tz":-600,"elapsed":3305,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}}},"source":["%cd drive/Colab Notebooks/drive"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/drive/Colab Notebooks/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iarCT5-ArC8s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"e30e6a44-05fc-46ac-d1ec-6a622416ca0d","executionInfo":{"status":"ok","timestamp":1558972380653,"user_tz":-600,"elapsed":3523,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}}},"source":["import keras\n","from keras.layers import *\n","from keras.activations import softmax\n","from keras.models import Model\n","from keras.layers.merge import concatenate\n","from keras.layers.normalization import BatchNormalization\n","\n","\n","def get_ESIM_model(nb_words, embedding_dim, embedding_matrix, recurrent_units, dense_units, dropout_rate, max_sequence_length, out_size):\n","    embedding_layer = Embedding(nb_words,\n","                                embedding_dim,\n","                                # embeddings_initializer='uniform',\n","                                weights=[embedding_matrix],\n","                                input_length=max_sequence_length,\n","                                trainable=False)\n","\n","    input_q1_layer = Input(shape=(max_sequence_length,), dtype='int32', name='q1')\n","    input_q2_layer = Input(shape=(max_sequence_length,), dtype='int32', name='q2')\n","\n","    embedding_sequence_q1 = BatchNormalization(axis=2)(embedding_layer(input_q1_layer))\n","    embedding_sequence_q2 = BatchNormalization(axis=2)(embedding_layer(input_q2_layer))\n","\n","    final_embedding_sequence_q1 = SpatialDropout1D(0.25)(embedding_sequence_q1)\n","    final_embedding_sequence_q2 = SpatialDropout1D(0.25)(embedding_sequence_q2)\n","\n","    rnn_layer_q1 = Bidirectional(LSTM(recurrent_units, return_sequences=True))(final_embedding_sequence_q1)\n","    rnn_layer_q2 = Bidirectional(LSTM(recurrent_units, return_sequences=True))(final_embedding_sequence_q2)\n","\n","    attention = Dot(axes=-1)([rnn_layer_q1, rnn_layer_q2])\n","    w_attn_1 = Lambda(lambda x: softmax(x, axis=1))(attention)\n","    w_attn_2 = Permute((2, 1))(Lambda(lambda x: softmax(x, axis=2))(attention))\n","    align_layer_1 = Dot(axes=1)([w_attn_1, rnn_layer_q1])\n","    align_layer_2 = Dot(axes=1)([w_attn_2, rnn_layer_q2])\n","\n","    subtract_layer_1 = subtract([rnn_layer_q1, align_layer_1])\n","    subtract_layer_2 = subtract([rnn_layer_q2, align_layer_2])\n","\n","    multiply_layer_1 = multiply([rnn_layer_q1, align_layer_1])\n","    multiply_layer_2 = multiply([rnn_layer_q2, align_layer_2])\n","\n","    m_q1 = concatenate([rnn_layer_q1, align_layer_1, subtract_layer_1, multiply_layer_1])\n","    m_q2 = concatenate([rnn_layer_q2, align_layer_2, subtract_layer_2, multiply_layer_2])\n","\n","    v_q1_i = Bidirectional(LSTM(recurrent_units, return_sequences=True))(m_q1)\n","    v_q2_i = Bidirectional(LSTM(recurrent_units, return_sequences=True))(m_q2)\n","\n","    avgpool_q1 = GlobalAveragePooling1D()(v_q1_i)\n","    avgpool_q2 = GlobalAveragePooling1D()(v_q2_i)\n","    maxpool_q1 = GlobalMaxPooling1D()(v_q1_i)\n","    maxpool_q2 = GlobalMaxPooling1D()(v_q2_i)\n","\n","    merged_q1 = concatenate([avgpool_q1, maxpool_q1])\n","    merged_q2 = concatenate([avgpool_q2, maxpool_q2])\n","\n","    final_v = BatchNormalization()(concatenate([merged_q1, merged_q2]))\n","    output = Dense(units=dense_units, activation='relu')(final_v)\n","    output = BatchNormalization()(output)\n","    output = Dropout(dropout_rate)(output)\n","    output = Dense(units=out_size, activation='sigmoid')(output)\n","\n","    model = Model(inputs=[input_q1_layer, input_q2_layer], output=output)\n","    adam_optimizer = keras.optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n","\n","    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['binary_crossentropy', 'accuracy'])\n","\n","    return model"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"PAWWqePxrG4h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"9f40339c-67ed-4dd6-ee59-de3e64ac5308","executionInfo":{"status":"ok","timestamp":1558972384272,"user_tz":-600,"elapsed":1689,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}}},"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","words_stop= stopwords.words('english')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nUjswtTHrHqE","colab_type":"code","colab":{}},"source":["keep_punctuation=True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"obgf_4Sp_8Lk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"44fe6198-2a29-42e9-d3d3-22b88e223fdc","executionInfo":{"status":"ok","timestamp":1558972452918,"user_tz":-600,"elapsed":838,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}}},"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","words_stop= stopwords.words('english')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GPo56ithrLu4","colab_type":"code","colab":{}},"source":["import numpy as np\n","from tqdm import tqdm\n","from string import punctuation as p\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from gensim.models import KeyedVectors\n","import json\n","import re\n","\n","class Dataloader(object):\n","    def __init__(self,clean_data=True, remove_stopwords=True):\n","        self.q1_data, self.q2_data, self.label = self.read_dataset(\"train_temp_wikiSent.json\")\n","        self.embedding_index = self.load_pretrain_embedding(\"model.txt\")\n","        self.word_num=len(self.embedding_index.wv.vocab)\n","        if clean_data:\n","            if remove_stopwords:\n","                self.ignored_word = words_stop\n","            self.cleaned_q1_data, self.cleaned_q2_data = [], []\n","            for text in self.q1_data:\n","                self.cleaned_q1_data.append(self.clean_data(text))\n","            for text in self.q2_data:\n","                self.cleaned_q2_data.append(self.clean_data(text))\n","        self.q1_sequences, self.q2_sequences, self.word_index = self.tokenizer()\n","        self.nb_words, self.embedding_matrix = self.prepare_embedding_matrix()\n","       \n","    def read_dataset(self, train_path):\n","        q1_data=[]\n","        q2_data=[]\n","        label=[]\n","        with open(train_path,'r') as f:\n","            file_content=json.load(f)\n","\n","        for key in file_content.keys():\n","            detail_content = file_content[key]\n","            q1_data.append (detail_content['claim'])\n","\n","            evidences=detail_content['evidence']\n","            temp_evidence=\"\"\n","            for evidence in evidences:\n","              temp_evidence.join(evidence[2])\n","            \n","            q2_data.append(temp_evidence)\n","\n","        label = np.array(label)    \n","        return q1_data, q2_data, label\n","\n","    def load_pretrain_embedding(self, file):\n","        print('Indexing word vector...')\n","        embedding_index = KeyedVectors.load_word2vec_format(file,binary=False)\n","\n","        return embedding_index\n","\n","    def clean_data(self, text):\n","        replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n","        try:\n","          text = text.lower()\n","        except:\n","          print(text)\n","        text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n","        text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n","        text = re.sub(r\"what's\", \"what is \", text)\n","        text = re.sub(r\"\\'s\", \" \", text)\n","        text = re.sub(r\"\\'ve\", \" have \", text)\n","        text = re.sub(r\"can't\", \"cannot \", text)\n","        text = re.sub(r\"n't\", \" not \", text)\n","        text = re.sub(r\"i'm\", \"i am \", text)\n","        text = re.sub(r\"i’m\", \"i am\", text)\n","        text = re.sub(r\"\\'re\", \" are \", text)\n","        text = re.sub(r\"\\'d\", \" would \", text)\n","        text = re.sub(r\"\\'ll\", \" will \", text)\n","        text = re.sub(r\"e - mail\", \"email\", text)\n","        text = re.sub(r\" +\", \"\", text)\n","\n","        stop_p = p + \"~·！@#￥%……&*（）——=+-{}【】：；“”‘’《》，。？、|、\"\n","\n","        if keep_punctuation:\n","            text = re.sub(r\"”\", \"\\\"\", text)\n","            text = re.sub(r\"“\", \"\\\"\", text)\n","            text = re.sub(r\"´\", \"'\", text)\n","            text = re.sub(r\"—\", \" \", text)\n","            text = re.sub(r\"’\", \"'\", text)\n","            text = re.sub(r\"‘\", \"'\", text)\n","            text = re.sub(r\",\", \" \", text)\n","            text = re.sub(r\"\\.\", \" \", text)\n","            text = re.sub(r\"!\", \" ! \", text)\n","            text = re.sub(r\"\\/\", \" \", text)\n","            text = re.sub(r\"\\^\", \" ^ \", text)\n","            text = re.sub(r\"\\+\", \" + \", text)\n","            text = re.sub(r\"\\-\", \" - \", text)\n","            text = re.sub(r\"\\=\", \" = \", text)\n","            text = re.sub(r\"'\", \" \", text)\n","            text = re.sub(r\":\", \" : \", text)\n","            text = re.sub(r\"−\", \" \", text)\n","            text = re.sub(r\"\\?\", \" ? \", text)\n","            text = re.sub(r\"\\^\", \" ^ \", text)\n","            text = re.sub(r\"#\", \" # \", text)\n","            text = re.sub(r\"￥\", \"$\", text)\n","        else:\n","            for token in stop_p:\n","                text = re.sub(token, \"\", text)\n","\n","        text = replace_numbers.sub('', text)\n","\n","        text = \"\".join([word for word in text if word not in self.ignored_word])\n","\n","        return text\n","\n","    def tokenizer(self):\n","        tokenizer = Tokenizer(num_words=None)\n","        tokenizer.fit_on_texts(self.cleaned_q1_data + self.cleaned_q2_data)\n","        q1_sequences = tokenizer.texts_to_sequences(self.cleaned_q1_data)\n","        q2_sequences = tokenizer.texts_to_sequences(self.cleaned_q2_data)\n","\n","        word_index = tokenizer.word_index\n","        print('Found %s unique tokens' % len(word_index))\n","\n","        # Padding\n","        q1_data = pad_sequences(q1_sequences, maxlen=60)\n","        print('Shape of q1_data tensor: ', q1_data.shape)\n","        q2_data = pad_sequences(q2_sequences, maxlen=60)\n","        print('Shape of q2_data tensor: ', q2_data.shape)\n","        print('Shape of label tensor: ', self.label.shape)\n","\n","        return q1_data, q2_data, word_index\n","\n","    def prepare_embedding_matrix(self):\n","        nb_words = len(self.word_index)\n","        embedding_matrix = np.zeros((nb_words + 1, 300))\n","\n","        print('Creating embedding matrix ...')\n","        for word, idx in self.word_index.items():\n","            if word in self.embedding_index.wv.vocab:\n","                embedding_vector = self.embedding_index.wv[word]\n","                embedding_matrix[idx] = embedding_vector\n","\n","        return nb_words, embedding_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dp81yeUgJcbH","colab_type":"code","colab":{}},"source":["class Testloader(object):\n","    def __init__(self,clean_data=True, remove_stopwords=True):\n","        self.q1_data, self.q2_data, self.label = self.read_dataset(\"test-unlabelled_evidence_0.8_siameseLSTM_epoch9_with_sentence.json\")\n","        self.embedding_index = self.load_pretrain_embedding(\"model.txt\")\n","        self.word_num=len(self.embedding_index.wv.vocab)\n","        if clean_data:\n","            if remove_stopwords:\n","                self.ignored_word = words_stop\n","            self.cleaned_q1_data, self.cleaned_q2_data = [], []\n","            for text in self.q1_data:\n","                self.cleaned_q1_data.append(self.clean_data(text))\n","            for text in self.q2_data:\n","                self.cleaned_q2_data.append(self.clean_data(text))\n","        self.q1_sequences, self.q2_sequences, self.word_index = self.tokenizer()\n","        self.nb_words, self.embedding_matrix = self.prepare_embedding_matrix()\n","       \n","    def read_dataset(self, train_path):\n","        q1_data=[]\n","        q2_data=[]\n","        label=[]\n","        with open(train_path,'r') as f:\n","            file_content=json.load(f)\n","\n","        for key in file_content.keys():\n","            detail_content = file_content[key]\n","            q1_data.append (detail_content['claim'])\n","\n","            evidences=detail_content['evidence']\n","            temp_evidence=\"\"\n","            for evidence in evidences:\n","              temp_evidence.join(evidence[2])\n","            \n","            q2_data.append(temp_evidence)\n","\n","        label = np.array(label)    \n","        return q1_data, q2_data, label\n","\n","    def load_pretrain_embedding(self, file):\n","        print('Indexing word vector...')\n","        embedding_index = KeyedVectors.load_word2vec_format(file,binary=False)\n","\n","        return embedding_index\n","\n","    def clean_data(self, text):\n","        replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n","        try:\n","          text = text.lower()\n","        except:\n","          print(text)\n","       \n","        text = \"\".join([word for word in text if word not in self.ignored_word])\n","\n","        return text\n","\n","    def tokenizer(self):\n","        tokenizer = Tokenizer(num_words=None)\n","        tokenizer.fit_on_texts(self.cleaned_q1_data + self.cleaned_q2_data)\n","        q1_sequences = tokenizer.texts_to_sequences(self.cleaned_q1_data)\n","        q2_sequences = tokenizer.texts_to_sequences(self.cleaned_q2_data)\n","\n","        word_index = tokenizer.word_index\n","        print('Found %s unique tokens' % len(word_index))\n","\n","        # Padding\n","        q1_data = pad_sequences(q1_sequences, maxlen=60)\n","        print('Shape of q1_data tensor: ', q1_data.shape)\n","        q2_data = pad_sequences(q2_sequences, maxlen=60)\n","        print('Shape of q2_data tensor: ', q2_data.shape)\n","        print('Shape of label tensor: ', self.label.shape)\n","\n","        return q1_data, q2_data, word_index\n","\n","    def prepare_embedding_matrix(self):\n","        nb_words = len(self.word_index)\n","        embedding_matrix = np.zeros((nb_words + 1, 300))\n","\n","        print('Creating embedding matrix ...')\n","        for word, idx in self.word_index.items():\n","            if word in self.embedding_index.wv.vocab:\n","                embedding_vector = self.embedding_index.wv[word]\n","                embedding_matrix[idx] = embedding_vector\n","\n","        return nb_words, embedding_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"81dlzO1CD2ei","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"c0710e61-e3fe-4e32-b642-7ed20ec654be","executionInfo":{"status":"ok","timestamp":1558975270220,"user_tz":-600,"elapsed":150152,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}}},"source":["data_loader = Dataloader()"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Indexing word vector...\n","Found 135360 unique tokens\n","Shape of q1_data tensor:  (145449, 60)\n","Shape of q2_data tensor:  (145449, 60)\n","Shape of label tensor:  (0,)\n","Creating embedding matrix ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ilJ9jvsHJm5y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"eccc7996-373d-4e16-d877-96ac8a825855","executionInfo":{"status":"ok","timestamp":1558975132535,"user_tz":-600,"elapsed":107912,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}}},"source":["test_loader=Testloader()"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Indexing word vector...\n","Found 15551 unique tokens\n","Shape of q1_data tensor:  (14997, 60)\n","Shape of q2_data tensor:  (14997, 60)\n","Shape of label tensor:  (0,)\n","Creating embedding matrix ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZQeTKYDzrRyf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"badde193-9620-4188-ab53-62f85fe203d7","executionInfo":{"status":"ok","timestamp":1558977530619,"user_tz":-600,"elapsed":795118,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}}},"source":["import warnings, os\n","from keras.models import load_model\n","warnings.filterwarnings('ignore')\n","\n","model_path='model_esimESIM_2.h5'\n","model = get_ESIM_model(data_loader.nb_words+1, 300, data_loader.embedding_matrix,\n","                           300, 300, 0.5, 60, 1)\n","\n","model.load_weights(model_path)\n","eval_predict = model.predict([test_loader.q1_sequences, test_loader.q2_sequences], \n","                                     batch_size=1024, verbose=1)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["14997/14997 [==============================] - 782s 52ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GTLvNn-IrX-K","colab_type":"code","colab":{}},"source":["with open(\"test-unlabelled_evidence_0.8_siameseLSTM_epoch9_with_sentence.json\",'r') as f:\n","      file_content=json.load(f)\n","      for i,key in enumerate(file_content.keys()):\n","          detail_content = file_content[key]\n","          \n","          if(eval_predict[i]==1):\n","              detail_content['label']='SUPPORTS'\n","          elif(eval_predict[i]==0):\n","              detail_content['label']='REFUTES'\n","          else:\n","              detail_content['label']='NOT ENOUGH INFO'\n","\n","          evidences=detail_content['evidence']\n","          \n","          if(evidences==[]):\n","            detail_content['label']='NOT ENOUGH INFO'\n","          for evidence in evidences:\n","            del evidence[2]\n","\n","with open('word2vec-result2.json','w') as t:\n","      json.dump(file_content, t, indent=2, separators=(',',':'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XDb-aIqIO0S0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"9dfbb383-179e-4751-fa26-91463280b127","executionInfo":{"status":"ok","timestamp":1558977921123,"user_tz":-600,"elapsed":1241,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}}},"source":["print(test_loader.embedding_matrix)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["[[ 0.        0.        0.       ...  0.        0.        0.      ]\n"," [ 0.064784 -0.094822  0.115844 ...  0.03993  -0.005045 -0.069976]\n"," [ 0.        0.        0.       ...  0.        0.        0.      ]\n"," ...\n"," [ 0.        0.        0.       ...  0.        0.        0.      ]\n"," [ 0.        0.        0.       ...  0.        0.        0.      ]\n"," [ 0.        0.        0.       ...  0.        0.        0.      ]]\n"],"name":"stdout"}]}]}