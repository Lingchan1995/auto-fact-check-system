{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"stage2 train.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x6O09qLuCzjn","colab_type":"text"},"source":["#this file is for training the evidence labelling\n","written by Yige Wen\n","\n","----"]},{"cell_type":"code","metadata":{"id":"jYVz_poSCZBQ","colab_type":"code","colab":{}},"source":["import torch \n","import torch.nn as nn\n","from torch.autograd import Variable\n","import numpy as np \n","import matplotlib.pyplot as plt \n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader \n","import csv\n","\n","import gensim\n","embedding_model = gensim.models.KeyedVectors.load_word2vec_format('word_embedding/model.txt',binary=False)\n","\n","import nltk\n","from nltk.corpus import stopwords\n","sw = stopwords.words('english')+['-LRB-', '-RRB-', '.', ',', '\"', '?', '!', \"'\", ':', '-LSB-', '-RSB-']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4msKghMiCZBU","colab_type":"code","colab":{}},"source":["class EvidencePairDataset(Dataset):\n","    def __init__(self, root, model):\n","        self.data = []\n","        self.model = model\n","        self.sw = sw\n","        with open(root) as csvfile: \n","            reader = csv.DictReader(csvfile) \n","#             self.i = 0\n","            for row in reader: \n","#                 if self.i == size:\n","#                     break\n","#                 self.i += 1\n","                claim_embedding = torch.tensor(words2sen(row['claim'], self.model, self.sw), dtype=torch.float)\n","                evidence_embedding = torch.tensor(words2sen(row['evidence'], self.model, self.sw), dtype=torch.float)\n","                if(claim_embedding.size(0) == 0 or evidence_embedding.size(0) == 0):\n","#                     print('empty sentence')\n","                    continue\n","                if row['label'] == '1':\n","                    label = 1\n","                elif row['label'] == '0':\n","                    label = 0\n","                # claim,row['claim'], row['evidence'], row['label']\n","#                 print(torch.FloatTensor(claim_embedding))\n","#                 print(torch.FloatTensor(evidence_embedding))\n","#                 print(label)\n","#                 self.data.append([torch.FloatTensor(claim_embedding), \n","#                                   torch.FloatTensor(evidence_embedding), \n","#                                   label]\n","#                                 )\n","                self.data.append((torch.FloatTensor(claim_embedding),\n","                                  torch.FloatTensor(evidence_embedding),\n","                                  label))\n","#         self.data = torch.stack()\n","        \n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        claim, evidence, label = self.data[index]\n","        return claim, evidence, label\n","\n","def words2sen(sen, model, stopwords, nearWordsNum = 5):\n","    words=[w for w in nltk.tokenize.word_tokenize(sen) if w not in stopwords]\n","    sen = []\n","    for i in range(len(words)):\n","        try:\n","            sen.append(model[words[i]])\n","            # print(\"有\", words[i])\n","        except:\n","            near = []\n","            for j in range(-nearWordsNum,nearWordsNum+1):\n","                if j >=0 and j < len(words):\n","                    nearIx = i+j\n","                    if nearIx != i:\n","                        near.append(words[j])\n","            \n","            near = [model[t] for t in near if t in model.vocab and t not in sw]\n","            if near != []:\n","                sumVec = 0\n","                for x in near:\n","                    sumVec += x\n","                sen.append(sumVec/len(near))\n","                # print(\"有邻居\", words[i])\n","            else:\n","                sen.append(-0.05+0.1*np.random.random(300))\n","                # print(\"纯碎机\", words[i])\n","    return sen"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jkmJxHJZCZBX","colab_type":"code","colab":{}},"source":["train_pair = EvidencePairDataset('train_pairs.csv', embedding_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbHWii1DCZBa","colab_type":"code","colab":{}},"source":["test_pair = EvidencePairDataset('devset_pairs.csv', embedding_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KQo1k65UCZBd","colab_type":"code","colab":{},"outputId":"391c6245-fdd2-4234-cec5-2ebaf9fdf2a0"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"torch.cuda.is_available()   =\", torch.cuda.is_available())\n","print(\"torch.cuda.device_count()   =\", torch.cuda.device_count())\n","print(\"torch.cuda.device('cuda')   =\", torch.cuda.device('cuda'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.cuda.is_available()   = True\n","torch.cuda.device_count()   = 1\n","torch.cuda.device('cuda')   = <torch.cuda.device object at 0x000001EB1FEB3A90>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fw8_CC7pCZBj","colab_type":"code","colab":{}},"source":["def collate_fn(batch):\n","    claims = [b[0] for b in batch]\n","    evidences = [b[1] for b in batch]\n","    labels = torch.tensor([b[2] for b in batch],dtype=torch.long)\n","    \n","    padded_claims, claim_lengths = padding(claims)\n","    padded_evidences, evidences_lengths = padding(evidences)\n","    \n","    return padded_claims, claim_lengths, padded_evidences, evidences_lengths, labels\n","\n","def padding(batch_one_field, features=300):\n","    max_claim_length = max([len(b) for b in batch_one_field])\n","    lengths = []\n","    padded = []\n","    for b in batch_one_field:\n","        lengths.append(len(b))\n","        b = torch.cat((b,torch.zeros(max_claim_length-len(b),features)),dim=0)\n","        padded.append(b)\n","    torch.FloatTensor(lengths)\n","    return torch.stack(padded), torch.LongTensor(lengths)\n","    \n","train_pair_loader = DataLoader(dataset = train_pair,\n","                               batch_size = 100,\n","                               shuffle = True,\n","                               collate_fn = collate_fn)\n","\n","def sort_sequences(inputs, lengths):\n","    \"\"\"sort_sequences\n","    Sort sequences according to lengths descendingly.\n","\n","    :param inputs (Tensor): input sequences, size [B, T, D]\n","    :param lengths (Tensor): length of each sequence, size [B]\n","    \"\"\"\n","    lengths_sorted, sorted_idx = lengths.sort(descending=True)\n","    _, unsorted_idx = sorted_idx.sort()\n","    return inputs[sorted_idx], lengths_sorted, unsorted_idx"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1AcBPs5ZCZBm","colab_type":"code","colab":{}},"source":["class Siamese_LSTM(nn.Module):\n","    def __init__(self):\n","        super(Siamese_LSTM, self).__init__()\n","        self.lstm = nn.LSTM(\n","            input_size = 300,\n","            hidden_size = 150,\n","            num_layers = 3,\n","            bidirectional = True,\n","            dropout = 0.3,\n","            batch_first = True\n","        )\n","\n","        self.out = nn.Sequential(\n","            nn.Linear(1200, 800),\n","            nn.Dropout(0.3),\n","            nn.ReLU(),\n","            nn.Linear(800, 400),\n","            nn.Dropout(0.2),\n","            nn.ReLU(),\n","            nn.Linear(400, 200),\n","            nn.Dropout(0.1),\n","            nn.ReLU(),\n","            nn.Linear(200, 2)\n","        )\n","\n","    def forward_once(self, x, lengths):\n","        inputs, sorted_lengths, unsorted_idx = sort_sequences(x,lengths)\n","#         print(inputs, sorted_lengths, unsorted_idx)\n","        inputs = torch.nn.utils.rnn.pack_padded_sequence(inputs, sorted_lengths, batch_first=True)\n"," \n","        outputs, _ = self.lstm(inputs, None)\n","\n","        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n","        outputs = outputs.index_select(0, unsorted_idx)\n","        output_tensor = []\n","#         print(lengths)\n","        for output,length in zip(outputs,lengths):\n","            output_tensor.append(output[length-1,:])\n","        return torch.stack(output_tensor)\n","        \n","    def forward(self, x1, len1, x2, len2):\n","        r_out1 = self.forward_once(x1, len1)\n","        r_out2 = self.forward_once(x2, len2)\n","        \n","        siamese_out = torch.cat((r_out1,r_out2,torch.abs(r_out1-r_out2),r_out1*r_out2),1)\n","        return self.out(siamese_out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsMdQce3CZBp","colab_type":"code","colab":{},"outputId":"67c0ba9e-04a4-4d95-df38-e2196c09502e"},"source":["net = Siamese_LSTM()\n","# net = torch.load('Siamese_LSTM-epoch9.pkl')\n","net.cuda()\n","print(net)\n","optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n","loss_func = nn.CrossEntropyLoss()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Siamese_LSTM(\n","  (lstm): LSTM(300, 150, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n","  (out): Sequential(\n","    (0): Linear(in_features=1200, out_features=800, bias=True)\n","    (1): Dropout(p=0.3)\n","    (2): ReLU()\n","    (3): Linear(in_features=800, out_features=400, bias=True)\n","    (4): Dropout(p=0.2)\n","    (5): ReLU()\n","    (6): Linear(in_features=400, out_features=200, bias=True)\n","    (7): Dropout(p=0.1)\n","    (8): ReLU()\n","    (9): Linear(in_features=200, out_features=2, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aj0fdElJCZBu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9wnR-Z3CZBx","colab_type":"code","colab":{}},"source":["def save_net(net, epoch):\n","    name = 'Siamese_BiLSTM_Dropout-epoch'+str(epoch)+'.pkl'\n","    torch.save(net,name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KME2vD7ACZB0","colab_type":"code","colab":{}},"source":["def test():\n","    net.eval()\n","    test_shuf = test_pair.data\n","    np.random.shuffle(test_shuf)\n","#     print(len(test_shuf))\n","    test_inputs1,test_lengths1,test_inputs2,test_lengths2,test_labels = collate_fn(test_shuf[:2000])\n","    test_inputs1 = Variable(test_inputs1).cuda()\n","    test_lengths1 = Variable(test_lengths1).cuda()\n","    test_inputs2 = Variable(test_inputs2).cuda()\n","    test_lengths2 = Variable(test_lengths2).cuda()\n","    test_labels = Variable(test_labels).cuda()\n","    test_output = net(test_inputs1, test_lengths1, test_inputs2, test_lengths2)\n","    test_pred_labels = torch.max(test_output, 1)[1].data.cpu().numpy()\n","    test_labels = test_labels.cpu().numpy()\n","    correct = 0\n","    for x,y in zip(test_pred_labels,test_labels):\n","        if x == y:\n","            correct += 1\n","    print('accuracy:',correct,'/',len(test_pred_labels),'=',correct/len(test_pred_labels))\n","    net.train()\n","\n","# test()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaAuSXmSCZB4","colab_type":"code","colab":{},"outputId":"3c489992-5fad-4339-ed48-721e5030b958"},"source":["EPOCH = 10\n","for epoch in range(15):\n","    for i, data in enumerate(train_pair_loader):\n","        inputs1, lengths1, inputs2, lengths2, labels = data\n","        inputs1 = Variable(inputs1).cuda()\n","        lengths1 = Variable(lengths1).cuda()\n","        inputs2 = Variable(inputs2).cuda()\n","        lengths2 = Variable(lengths2).cuda()\n","        labels = Variable(labels).cuda()\n","        output = net(inputs1, lengths1, inputs2, lengths2)\n","        \n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # forward + backward + optimize\n","        loss = loss_func(output, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if i%100 == 0:\n","            print('epoch:',epoch,'batch',i,'loss:',loss.item())\n","            test()\n","            \n","    save_net(net, epoch)\n","        \n","    print('epoch:',epoch,'loss:',loss.item())\n","\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch: 0 batch 0 loss: 0.6963233947753906\n","accuracy: 990 / 2000 = 0.495\n","epoch: 0 batch 100 loss: 0.6957831382751465\n","accuracy: 1011 / 2000 = 0.5055\n","epoch: 0 batch 200 loss: 0.6965621113777161\n","accuracy: 1269 / 2000 = 0.6345\n","epoch: 0 batch 300 loss: 0.6278018951416016\n","accuracy: 1274 / 2000 = 0.637\n","epoch: 0 batch 400 loss: 0.6134536266326904\n","accuracy: 1279 / 2000 = 0.6395\n","epoch: 0 batch 500 loss: 0.547346830368042\n","accuracy: 1295 / 2000 = 0.6475\n","epoch: 0 batch 600 loss: 0.6389167308807373\n","accuracy: 1275 / 2000 = 0.6375\n","epoch: 0 batch 700 loss: 0.5235599875450134\n","accuracy: 1278 / 2000 = 0.639\n","epoch: 0 batch 800 loss: 0.6384148597717285\n","accuracy: 1322 / 2000 = 0.661\n","epoch: 0 batch 900 loss: 0.6140878796577454\n","accuracy: 1315 / 2000 = 0.6575\n","epoch: 0 batch 1000 loss: 0.6745547652244568\n","accuracy: 1307 / 2000 = 0.6535\n","epoch: 0 batch 1100 loss: 0.6853716373443604\n","accuracy: 1307 / 2000 = 0.6535\n","epoch: 0 batch 1200 loss: 0.5761851072311401\n","accuracy: 1288 / 2000 = 0.644\n","epoch: 0 batch 1300 loss: 0.5814382433891296\n","accuracy: 1302 / 2000 = 0.651\n","epoch: 0 batch 1400 loss: 0.5806264281272888\n","accuracy: 1330 / 2000 = 0.665\n","epoch: 0 batch 1500 loss: 0.5393126606941223\n","accuracy: 1380 / 2000 = 0.69\n","epoch: 0 batch 1600 loss: 0.5474417209625244\n","accuracy: 1341 / 2000 = 0.6705\n","epoch: 0 batch 1700 loss: 0.5753641724586487\n","accuracy: 1311 / 2000 = 0.6555\n","epoch: 0 batch 1800 loss: 0.5778510570526123\n","accuracy: 1349 / 2000 = 0.6745\n","epoch: 0 batch 1900 loss: 0.5746254920959473\n","accuracy: 1354 / 2000 = 0.677\n","epoch: 0 batch 2000 loss: 0.619661271572113\n","accuracy: 1356 / 2000 = 0.678\n","epoch: 0 batch 2100 loss: 0.496246337890625\n","accuracy: 1370 / 2000 = 0.685\n","epoch: 0 batch 2200 loss: 0.6404466032981873\n","accuracy: 1345 / 2000 = 0.6725\n","epoch: 0 batch 2300 loss: 0.6040674448013306\n","accuracy: 1372 / 2000 = 0.686\n","epoch: 0 batch 2400 loss: 0.5462852716445923\n","accuracy: 1370 / 2000 = 0.685\n","epoch: 0 batch 2500 loss: 0.5611317157745361\n","accuracy: 1387 / 2000 = 0.6935\n","epoch: 0 batch 2600 loss: 0.5287865400314331\n","accuracy: 1401 / 2000 = 0.7005\n","epoch: 0 batch 2700 loss: 0.5365726351737976\n","accuracy: 1355 / 2000 = 0.6775\n","epoch: 0 batch 2800 loss: 0.5749486088752747\n","accuracy: 1414 / 2000 = 0.707\n","epoch: 0 batch 2900 loss: 0.5429533123970032\n","accuracy: 1380 / 2000 = 0.69\n","epoch: 0 batch 3000 loss: 0.5814385414123535\n","accuracy: 1383 / 2000 = 0.6915\n","epoch: 0 batch 3100 loss: 0.6528191566467285\n","accuracy: 1372 / 2000 = 0.686\n","epoch: 0 batch 3200 loss: 0.6211256980895996\n","accuracy: 1414 / 2000 = 0.707\n","epoch: 0 batch 3300 loss: 0.544753909111023\n","accuracy: 1384 / 2000 = 0.692\n","epoch: 0 batch 3400 loss: 0.5328859090805054\n","accuracy: 1382 / 2000 = 0.691\n","epoch: 0 batch 3500 loss: 0.5451066493988037\n","accuracy: 1389 / 2000 = 0.6945\n","epoch: 0 batch 3600 loss: 0.5684341192245483\n","accuracy: 1435 / 2000 = 0.7175\n","epoch: 0 batch 3700 loss: 0.5924318432807922\n","accuracy: 1398 / 2000 = 0.699\n"],"name":"stdout"},{"output_type":"stream","text":["D:\\yigewen\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Siamese_LSTM. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["epoch: 0 loss: 0.5895117521286011\n","epoch: 1 batch 0 loss: 0.6099227666854858\n","accuracy: 1414 / 2000 = 0.707\n","epoch: 1 batch 100 loss: 0.5336961150169373\n","accuracy: 1416 / 2000 = 0.708\n","epoch: 1 batch 200 loss: 0.5532849431037903\n","accuracy: 1411 / 2000 = 0.7055\n","epoch: 1 batch 300 loss: 0.5782852172851562\n","accuracy: 1416 / 2000 = 0.708\n","epoch: 1 batch 400 loss: 0.5584542751312256\n","accuracy: 1410 / 2000 = 0.705\n","epoch: 1 batch 500 loss: 0.5248233675956726\n","accuracy: 1424 / 2000 = 0.712\n","epoch: 1 batch 600 loss: 0.541163444519043\n","accuracy: 1423 / 2000 = 0.7115\n","epoch: 1 batch 700 loss: 0.539895236492157\n","accuracy: 1422 / 2000 = 0.711\n","epoch: 1 batch 800 loss: 0.5569810271263123\n","accuracy: 1421 / 2000 = 0.7105\n","epoch: 1 batch 900 loss: 0.5186588168144226\n","accuracy: 1417 / 2000 = 0.7085\n","epoch: 1 batch 1000 loss: 0.6017690300941467\n","accuracy: 1447 / 2000 = 0.7235\n","epoch: 1 batch 1100 loss: 0.5103204846382141\n","accuracy: 1413 / 2000 = 0.7065\n","epoch: 1 batch 1200 loss: 0.5293477773666382\n","accuracy: 1437 / 2000 = 0.7185\n","epoch: 1 batch 1300 loss: 0.5538351535797119\n","accuracy: 1411 / 2000 = 0.7055\n","epoch: 1 batch 1400 loss: 0.594731867313385\n","accuracy: 1422 / 2000 = 0.711\n","epoch: 1 batch 1500 loss: 0.5319413542747498\n","accuracy: 1444 / 2000 = 0.722\n","epoch: 1 batch 1600 loss: 0.5350174903869629\n","accuracy: 1455 / 2000 = 0.7275\n","epoch: 1 batch 1700 loss: 0.5681937336921692\n","accuracy: 1432 / 2000 = 0.716\n","epoch: 1 batch 1800 loss: 0.5913370251655579\n","accuracy: 1462 / 2000 = 0.731\n","epoch: 1 batch 1900 loss: 0.5163980722427368\n","accuracy: 1452 / 2000 = 0.726\n","epoch: 1 batch 2000 loss: 0.5618570446968079\n","accuracy: 1433 / 2000 = 0.7165\n","epoch: 1 batch 2100 loss: 0.5904578566551208\n","accuracy: 1469 / 2000 = 0.7345\n","epoch: 1 batch 2200 loss: 0.5962746143341064\n","accuracy: 1445 / 2000 = 0.7225\n","epoch: 1 batch 2300 loss: 0.4640839397907257\n","accuracy: 1447 / 2000 = 0.7235\n","epoch: 1 batch 2400 loss: 0.5195939540863037\n","accuracy: 1480 / 2000 = 0.74\n","epoch: 1 batch 2500 loss: 0.5657888054847717\n","accuracy: 1482 / 2000 = 0.741\n","epoch: 1 batch 2600 loss: 0.5157812237739563\n","accuracy: 1442 / 2000 = 0.721\n","epoch: 1 batch 2700 loss: 0.5080698132514954\n","accuracy: 1452 / 2000 = 0.726\n","epoch: 1 batch 2800 loss: 0.4976397752761841\n","accuracy: 1471 / 2000 = 0.7355\n","epoch: 1 batch 2900 loss: 0.5139224529266357\n","accuracy: 1427 / 2000 = 0.7135\n","epoch: 1 batch 3000 loss: 0.5679931044578552\n","accuracy: 1423 / 2000 = 0.7115\n","epoch: 1 batch 3100 loss: 0.5459710359573364\n","accuracy: 1420 / 2000 = 0.71\n","epoch: 1 batch 3200 loss: 0.47551780939102173\n","accuracy: 1461 / 2000 = 0.7305\n","epoch: 1 batch 3300 loss: 0.569204568862915\n","accuracy: 1443 / 2000 = 0.7215\n","epoch: 1 batch 3400 loss: 0.4653504490852356\n","accuracy: 1463 / 2000 = 0.7315\n","epoch: 1 batch 3500 loss: 0.49165236949920654\n","accuracy: 1452 / 2000 = 0.726\n","epoch: 1 batch 3600 loss: 0.5334936380386353\n","accuracy: 1459 / 2000 = 0.7295\n","epoch: 1 batch 3700 loss: 0.5792531371116638\n","accuracy: 1454 / 2000 = 0.727\n","epoch: 1 loss: 0.4717966616153717\n","epoch: 2 batch 0 loss: 0.41528648138046265\n","accuracy: 1477 / 2000 = 0.7385\n","epoch: 2 batch 100 loss: 0.4410667419433594\n","accuracy: 1470 / 2000 = 0.735\n","epoch: 2 batch 200 loss: 0.5086435079574585\n","accuracy: 1472 / 2000 = 0.736\n","epoch: 2 batch 300 loss: 0.5124260783195496\n","accuracy: 1424 / 2000 = 0.712\n","epoch: 2 batch 400 loss: 0.554414689540863\n","accuracy: 1467 / 2000 = 0.7335\n","epoch: 2 batch 500 loss: 0.5629594326019287\n","accuracy: 1473 / 2000 = 0.7365\n","epoch: 2 batch 600 loss: 0.46760666370391846\n","accuracy: 1491 / 2000 = 0.7455\n","epoch: 2 batch 700 loss: 0.659090518951416\n","accuracy: 1446 / 2000 = 0.723\n","epoch: 2 batch 800 loss: 0.5478828549385071\n","accuracy: 1427 / 2000 = 0.7135\n","epoch: 2 batch 900 loss: 0.542243480682373\n","accuracy: 1479 / 2000 = 0.7395\n","epoch: 2 batch 1000 loss: 0.5268327593803406\n","accuracy: 1474 / 2000 = 0.737\n","epoch: 2 batch 1100 loss: 0.49634456634521484\n","accuracy: 1498 / 2000 = 0.749\n","epoch: 2 batch 1200 loss: 0.475350946187973\n","accuracy: 1485 / 2000 = 0.7425\n","epoch: 2 batch 1300 loss: 0.49738845229148865\n","accuracy: 1470 / 2000 = 0.735\n","epoch: 2 batch 1400 loss: 0.6039940118789673\n","accuracy: 1469 / 2000 = 0.7345\n","epoch: 2 batch 1500 loss: 0.45085784792900085\n","accuracy: 1489 / 2000 = 0.7445\n","epoch: 2 batch 1600 loss: 0.48900607228279114\n","accuracy: 1487 / 2000 = 0.7435\n","epoch: 2 batch 1700 loss: 0.5883238911628723\n","accuracy: 1507 / 2000 = 0.7535\n","epoch: 2 batch 1800 loss: 0.5243895649909973\n","accuracy: 1477 / 2000 = 0.7385\n","epoch: 2 batch 1900 loss: 0.46949535608291626\n","accuracy: 1518 / 2000 = 0.759\n","epoch: 2 batch 2000 loss: 0.4847734570503235\n","accuracy: 1481 / 2000 = 0.7405\n","epoch: 2 batch 2100 loss: 0.46702495217323303\n","accuracy: 1501 / 2000 = 0.7505\n","epoch: 2 batch 2200 loss: 0.5014467835426331\n","accuracy: 1481 / 2000 = 0.7405\n","epoch: 2 batch 2300 loss: 0.5422462224960327\n","accuracy: 1489 / 2000 = 0.7445\n","epoch: 2 batch 2400 loss: 0.5416541695594788\n","accuracy: 1494 / 2000 = 0.747\n","epoch: 2 batch 2500 loss: 0.49883192777633667\n","accuracy: 1510 / 2000 = 0.755\n","epoch: 2 batch 2600 loss: 0.5238134264945984\n","accuracy: 1482 / 2000 = 0.741\n","epoch: 2 batch 2700 loss: 0.5523324012756348\n","accuracy: 1499 / 2000 = 0.7495\n","epoch: 2 batch 2800 loss: 0.5159153938293457\n","accuracy: 1476 / 2000 = 0.738\n","epoch: 2 batch 2900 loss: 0.46795180439949036\n","accuracy: 1489 / 2000 = 0.7445\n","epoch: 2 batch 3000 loss: 0.5089712738990784\n","accuracy: 1504 / 2000 = 0.752\n","epoch: 2 batch 3100 loss: 0.5146272778511047\n","accuracy: 1487 / 2000 = 0.7435\n","epoch: 2 batch 3200 loss: 0.4843858480453491\n","accuracy: 1469 / 2000 = 0.7345\n","epoch: 2 batch 3300 loss: 0.4720976948738098\n","accuracy: 1458 / 2000 = 0.729\n","epoch: 2 batch 3400 loss: 0.43176472187042236\n","accuracy: 1513 / 2000 = 0.7565\n","epoch: 2 batch 3500 loss: 0.5447812080383301\n","accuracy: 1492 / 2000 = 0.746\n","epoch: 2 batch 3600 loss: 0.46870657801628113\n","accuracy: 1489 / 2000 = 0.7445\n","epoch: 2 batch 3700 loss: 0.48853835463523865\n","accuracy: 1538 / 2000 = 0.769\n","epoch: 2 loss: 1.0915485620498657\n","epoch: 3 batch 0 loss: 0.5093084573745728\n","accuracy: 1498 / 2000 = 0.749\n","epoch: 3 batch 100 loss: 0.5096333026885986\n","accuracy: 1501 / 2000 = 0.7505\n","epoch: 3 batch 200 loss: 0.5229834914207458\n","accuracy: 1497 / 2000 = 0.7485\n","epoch: 3 batch 300 loss: 0.5390073657035828\n","accuracy: 1497 / 2000 = 0.7485\n","epoch: 3 batch 400 loss: 0.43558254837989807\n","accuracy: 1462 / 2000 = 0.731\n","epoch: 3 batch 500 loss: 0.5111550688743591\n","accuracy: 1495 / 2000 = 0.7475\n","epoch: 3 batch 600 loss: 0.5081873536109924\n","accuracy: 1507 / 2000 = 0.7535\n","epoch: 3 batch 700 loss: 0.46924906969070435\n","accuracy: 1512 / 2000 = 0.756\n","epoch: 3 batch 800 loss: 0.35518306493759155\n","accuracy: 1455 / 2000 = 0.7275\n","epoch: 3 batch 900 loss: 0.44145524501800537\n","accuracy: 1497 / 2000 = 0.7485\n","epoch: 3 batch 1000 loss: 0.4801480174064636\n","accuracy: 1467 / 2000 = 0.7335\n","epoch: 3 batch 1100 loss: 0.537368893623352\n","accuracy: 1467 / 2000 = 0.7335\n","epoch: 3 batch 1200 loss: 0.5878713726997375\n","accuracy: 1497 / 2000 = 0.7485\n","epoch: 3 batch 1300 loss: 0.4390806555747986\n","accuracy: 1482 / 2000 = 0.741\n","epoch: 3 batch 1400 loss: 0.45010966062545776\n","accuracy: 1532 / 2000 = 0.766\n","epoch: 3 batch 1500 loss: 0.4656277000904083\n","accuracy: 1492 / 2000 = 0.746\n","epoch: 3 batch 1600 loss: 0.574530839920044\n","accuracy: 1522 / 2000 = 0.761\n","epoch: 3 batch 1700 loss: 0.4891701638698578\n","accuracy: 1488 / 2000 = 0.744\n","epoch: 3 batch 1800 loss: 0.38331082463264465\n","accuracy: 1533 / 2000 = 0.7665\n","epoch: 3 batch 1900 loss: 0.422435462474823\n","accuracy: 1476 / 2000 = 0.738\n","epoch: 3 batch 2000 loss: 0.4907039403915405\n","accuracy: 1496 / 2000 = 0.748\n","epoch: 3 batch 2100 loss: 0.563816249370575\n","accuracy: 1489 / 2000 = 0.7445\n","epoch: 3 batch 2200 loss: 0.5045880675315857\n","accuracy: 1506 / 2000 = 0.753\n","epoch: 3 batch 2300 loss: 0.4833369851112366\n","accuracy: 1534 / 2000 = 0.767\n","epoch: 3 batch 2400 loss: 0.5635085701942444\n","accuracy: 1478 / 2000 = 0.739\n","epoch: 3 batch 2500 loss: 0.45258674025535583\n","accuracy: 1509 / 2000 = 0.7545\n","epoch: 3 batch 2600 loss: 0.472664475440979\n","accuracy: 1498 / 2000 = 0.749\n","epoch: 3 batch 2700 loss: 0.4242161214351654\n","accuracy: 1516 / 2000 = 0.758\n","epoch: 3 batch 2800 loss: 0.40467631816864014\n","accuracy: 1462 / 2000 = 0.731\n","epoch: 3 batch 2900 loss: 0.43216046690940857\n","accuracy: 1525 / 2000 = 0.7625\n","epoch: 3 batch 3000 loss: 0.4829856753349304\n","accuracy: 1489 / 2000 = 0.7445\n","epoch: 3 batch 3100 loss: 0.4703293740749359\n","accuracy: 1522 / 2000 = 0.761\n","epoch: 3 batch 3200 loss: 0.4375019967556\n","accuracy: 1549 / 2000 = 0.7745\n","epoch: 3 batch 3300 loss: 0.40595608949661255\n","accuracy: 1505 / 2000 = 0.7525\n","epoch: 3 batch 3400 loss: 0.5684362053871155\n","accuracy: 1532 / 2000 = 0.766\n","epoch: 3 batch 3500 loss: 0.458830863237381\n","accuracy: 1539 / 2000 = 0.7695\n","epoch: 3 batch 3600 loss: 0.5360312461853027\n","accuracy: 1509 / 2000 = 0.7545\n","epoch: 3 batch 3700 loss: 0.48655396699905396\n","accuracy: 1537 / 2000 = 0.7685\n","epoch: 3 loss: 0.5578737854957581\n","epoch: 4 batch 0 loss: 0.5465114116668701\n","accuracy: 1500 / 2000 = 0.75\n","epoch: 4 batch 100 loss: 0.4671369791030884\n","accuracy: 1495 / 2000 = 0.7475\n","epoch: 4 batch 200 loss: 0.5654411315917969\n","accuracy: 1495 / 2000 = 0.7475\n","epoch: 4 batch 300 loss: 0.3692520260810852\n","accuracy: 1524 / 2000 = 0.762\n","epoch: 4 batch 400 loss: 0.5183981657028198\n","accuracy: 1529 / 2000 = 0.7645\n","epoch: 4 batch 500 loss: 0.44833439588546753\n","accuracy: 1525 / 2000 = 0.7625\n","epoch: 4 batch 600 loss: 0.51142817735672\n","accuracy: 1506 / 2000 = 0.753\n","epoch: 4 batch 700 loss: 0.4005569815635681\n","accuracy: 1530 / 2000 = 0.765\n","epoch: 4 batch 800 loss: 0.4189586639404297\n","accuracy: 1480 / 2000 = 0.74\n","epoch: 4 batch 900 loss: 0.5404760837554932\n","accuracy: 1515 / 2000 = 0.7575\n","epoch: 4 batch 1000 loss: 0.5151088833808899\n","accuracy: 1520 / 2000 = 0.76\n","epoch: 4 batch 1100 loss: 0.5705944299697876\n","accuracy: 1504 / 2000 = 0.752\n","epoch: 4 batch 1200 loss: 0.402614951133728\n","accuracy: 1515 / 2000 = 0.7575\n","epoch: 4 batch 1300 loss: 0.38629376888275146\n","accuracy: 1528 / 2000 = 0.764\n","epoch: 4 batch 1400 loss: 0.41971808671951294\n","accuracy: 1496 / 2000 = 0.748\n","epoch: 4 batch 1500 loss: 0.4489406645298004\n","accuracy: 1507 / 2000 = 0.7535\n","epoch: 4 batch 1600 loss: 0.48433905839920044\n","accuracy: 1548 / 2000 = 0.774\n","epoch: 4 batch 1700 loss: 0.4167822301387787\n","accuracy: 1505 / 2000 = 0.7525\n","epoch: 4 batch 1800 loss: 0.5377564430236816\n","accuracy: 1523 / 2000 = 0.7615\n","epoch: 4 batch 1900 loss: 0.45415765047073364\n","accuracy: 1554 / 2000 = 0.777\n","epoch: 4 batch 2000 loss: 0.44641387462615967\n","accuracy: 1525 / 2000 = 0.7625\n","epoch: 4 batch 2100 loss: 0.5320179462432861\n","accuracy: 1531 / 2000 = 0.7655\n","epoch: 4 batch 2200 loss: 0.46135467290878296\n","accuracy: 1568 / 2000 = 0.784\n","epoch: 4 batch 2300 loss: 0.42624741792678833\n","accuracy: 1548 / 2000 = 0.774\n","epoch: 4 batch 2400 loss: 0.4196060299873352\n","accuracy: 1501 / 2000 = 0.7505\n","epoch: 4 batch 2500 loss: 0.390702486038208\n","accuracy: 1504 / 2000 = 0.752\n","epoch: 4 batch 2600 loss: 0.6662757396697998\n","accuracy: 1514 / 2000 = 0.757\n","epoch: 4 batch 2700 loss: 0.3614831566810608\n","accuracy: 1515 / 2000 = 0.7575\n","epoch: 4 batch 2800 loss: 0.4910055100917816\n","accuracy: 1529 / 2000 = 0.7645\n","epoch: 4 batch 2900 loss: 0.4593573808670044\n","accuracy: 1550 / 2000 = 0.775\n","epoch: 4 batch 3000 loss: 0.4832864999771118\n","accuracy: 1510 / 2000 = 0.755\n","epoch: 4 batch 3100 loss: 0.4804019629955292\n","accuracy: 1541 / 2000 = 0.7705\n","epoch: 4 batch 3200 loss: 0.46157917380332947\n","accuracy: 1537 / 2000 = 0.7685\n","epoch: 4 batch 3300 loss: 0.38932210206985474\n","accuracy: 1504 / 2000 = 0.752\n","epoch: 4 batch 3400 loss: 0.5336765050888062\n","accuracy: 1538 / 2000 = 0.769\n","epoch: 4 batch 3500 loss: 0.482819139957428\n","accuracy: 1531 / 2000 = 0.7655\n","epoch: 4 batch 3600 loss: 0.7013862729072571\n","accuracy: 1499 / 2000 = 0.7495\n","epoch: 4 batch 3700 loss: 0.5480422377586365\n","accuracy: 1517 / 2000 = 0.7585\n","epoch: 4 loss: 0.5549697875976562\n","epoch: 5 batch 0 loss: 0.43802410364151\n","accuracy: 1535 / 2000 = 0.7675\n","epoch: 5 batch 100 loss: 0.3804803192615509\n","accuracy: 1512 / 2000 = 0.756\n","epoch: 5 batch 200 loss: 0.4741402864456177\n","accuracy: 1505 / 2000 = 0.7525\n","epoch: 5 batch 300 loss: 0.4773455858230591\n","accuracy: 1532 / 2000 = 0.766\n","epoch: 5 batch 400 loss: 0.4345950782299042\n","accuracy: 1539 / 2000 = 0.7695\n","epoch: 5 batch 500 loss: 0.5140731930732727\n","accuracy: 1521 / 2000 = 0.7605\n","epoch: 5 batch 600 loss: 0.41643035411834717\n","accuracy: 1524 / 2000 = 0.762\n","epoch: 5 batch 700 loss: 0.42633092403411865\n","accuracy: 1509 / 2000 = 0.7545\n","epoch: 5 batch 800 loss: 0.4182865619659424\n","accuracy: 1518 / 2000 = 0.759\n","epoch: 5 batch 900 loss: 0.4516719877719879\n","accuracy: 1485 / 2000 = 0.7425\n","epoch: 5 batch 1000 loss: 0.4092799127101898\n","accuracy: 1510 / 2000 = 0.755\n","epoch: 5 batch 1100 loss: 0.6026773452758789\n","accuracy: 1541 / 2000 = 0.7705\n","epoch: 5 batch 1200 loss: 0.3978215157985687\n","accuracy: 1507 / 2000 = 0.7535\n","epoch: 5 batch 1300 loss: 0.39396312832832336\n","accuracy: 1510 / 2000 = 0.755\n","epoch: 5 batch 1400 loss: 0.4977444112300873\n","accuracy: 1503 / 2000 = 0.7515\n","epoch: 5 batch 1500 loss: 0.4317063093185425\n","accuracy: 1541 / 2000 = 0.7705\n","epoch: 5 batch 1600 loss: 0.5376228094100952\n","accuracy: 1522 / 2000 = 0.761\n","epoch: 5 batch 1700 loss: 0.43130287528038025\n","accuracy: 1508 / 2000 = 0.754\n","epoch: 5 batch 1800 loss: 0.43740883469581604\n","accuracy: 1525 / 2000 = 0.7625\n","epoch: 5 batch 1900 loss: 0.46357041597366333\n","accuracy: 1510 / 2000 = 0.755\n","epoch: 5 batch 2000 loss: 0.4218641221523285\n","accuracy: 1547 / 2000 = 0.7735\n","epoch: 5 batch 2100 loss: 0.32322001457214355\n","accuracy: 1549 / 2000 = 0.7745\n","epoch: 5 batch 2200 loss: 0.5402815341949463\n","accuracy: 1523 / 2000 = 0.7615\n","epoch: 5 batch 2300 loss: 0.4608857035636902\n","accuracy: 1521 / 2000 = 0.7605\n","epoch: 5 batch 2400 loss: 0.40394145250320435\n","accuracy: 1495 / 2000 = 0.7475\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-16-66e3991ab4c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mEPOCH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pair_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0minputs1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0minputs1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mD:\\yigewen\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-7-5f7c68c6f5b9>\u001b[0m in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpadded_claims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclaim_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclaims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mpadded_evidences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevidences_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevidences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpadded_claims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclaim_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_evidences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevidences_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-7-5f7c68c6f5b9>\u001b[0m in \u001b[0;36mpadding\u001b[1;34m(batch_one_field, features)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_one_field\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mlengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_claim_length\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mpadded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"IvTtQ3JRCZB8","colab_type":"code","colab":{}},"source":["# print(len(test_inputs1))\n","# print(len(test_lengths1))\n","# print(len(test_inputs2))\n","# print(len(test_lengths2))\n","# print(len(test_labels))\n","\n","print(len(test_labels))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MAn_RpshCZB_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dvAZsoAjCZCC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zcl75WCvCZCJ","colab_type":"code","colab":{}},"source":["correct = 0\n","for x,y in zip(test_pred_labels,test_labels):\n","    if x == y:\n","        correct += 1\n","print(correct)\n","print('accuracy:',correct/len(test_pred_labels))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dEsGSbSRCZCM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}