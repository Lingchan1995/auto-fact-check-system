{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Enhanced_ESIM.ipyn","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_xoFslNU_GYQ","colab_type":"text"},"source":["#this file does Enhance_ESIM model \n","including training and predicting\n","\n","written by Linghan Zhou\n","\n","---"]},{"cell_type":"code","metadata":{"id":"AdmlOSNY2CJH","colab_type":"code","outputId":"c4b52164-d33d-49ea-ccc4-be9dc6080b49","executionInfo":{"status":"ok","timestamp":1559032990179,"user_tz":-600,"elapsed":1617,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import keras\n","from keras.layers import *\n","from keras.activations import softmax\n","from keras.models import Model\n","from keras.layers.merge import concatenate\n","from keras.layers.normalization import BatchNormalization\n","\n","\n","def get_ESIM_model(nb_words, embedding_dim, embedding_matrix, recurrent_units, dense_units, dropout_rate, max_sequence_length, out_size,name_value):\n","    embedding_layer = Embedding(nb_words,\n","                                embedding_dim,\n","                                # embeddings_initializer='uniform',\n","                                weights=[embedding_matrix],\n","                                input_length=max_sequence_length,                   \n","                                trainable=False)\n","\n","    input_q1_layer = Input(shape=(max_sequence_length,), dtype='int32', name=name_value+\"1\")\n","    input_q2_layer = Input(shape=(max_sequence_length,), dtype='int32', name=name_value+\"2\")\n","\n","    embedding_sequence_q1 = BatchNormalization(axis=2)(embedding_layer(input_q1_layer))\n","    embedding_sequence_q2 = BatchNormalization(axis=2)(embedding_layer(input_q2_layer))\n","\n","    final_embedding_sequence_q1 = SpatialDropout1D(0.25)(embedding_sequence_q1)\n","    final_embedding_sequence_q2 = SpatialDropout1D(0.25)(embedding_sequence_q2)\n","\n","    rnn_layer_q1 = Bidirectional(LSTM(recurrent_units, return_sequences=True))(final_embedding_sequence_q1)\n","    rnn_layer_q2 = Bidirectional(LSTM(recurrent_units, return_sequences=True))(final_embedding_sequence_q2)\n","\n","    attention = Dot(axes=-1)([rnn_layer_q1, rnn_layer_q2])\n","    w_attn_1 = Lambda(lambda x: softmax(x, axis=1))(attention)\n","    w_attn_2 = Permute((2, 1))(Lambda(lambda x: softmax(x, axis=2))(attention))\n","    align_layer_1 = Dot(axes=1)([w_attn_1, rnn_layer_q1])\n","    align_layer_2 = Dot(axes=1)([w_attn_2, rnn_layer_q2])\n","\n","    subtract_layer_1 = subtract([rnn_layer_q1, align_layer_1])\n","    subtract_layer_2 = subtract([rnn_layer_q2, align_layer_2])\n","\n","    multiply_layer_1 = multiply([rnn_layer_q1, align_layer_1])\n","    multiply_layer_2 = multiply([rnn_layer_q2, align_layer_2])\n","\n","    m_q1 = concatenate([rnn_layer_q1, align_layer_1, subtract_layer_1, multiply_layer_1])\n","    m_q2 = concatenate([rnn_layer_q2, align_layer_2, subtract_layer_2, multiply_layer_2])\n","\n","    v_q1_i = Bidirectional(LSTM(recurrent_units, return_sequences=True))(m_q1)\n","    v_q2_i = Bidirectional(LSTM(recurrent_units, return_sequences=True))(m_q2)\n","\n","    final_v = BatchNormalization()(concatenate([v_q1_i, v_q2_i]))\n","    input_layer=[input_q1_layer,input_q2_layer]\n","    \n","    return input_layer, final_v"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"kyn6wzhPWGhx","colab_type":"code","colab":{}},"source":["def get_enhanced(nb_words, embedding_dim, embedding_matrix, recurrent_units, dense_units, dropout_rate, max_sequence_length, out_size):\n","  tils=[] \n","  fils=[]\n","  for i in range(5):\n","    name=str(i+48)\n","    input_layer,output_layer=get_ESIM_model(nb_words, embedding_dim, embedding_matrix, recurrent_units, dense_units, dropout_rate, max_sequence_length, out_size,name)\n","    tils+=(input_layer)\n","    fils.append(output_layer)\n","\n","  models=concatenate(fils) \n","  print(nb_words) \n","  avgpool=GlobalAveragePooling1D()(models)\n","  maxpool=GlobalMaxPooling1D()(models)\n","  output = Dense(units=dense_units, activation='relu')(concatenate([avgpool,maxpool]))\n","  output = BatchNormalization()(output)\n","  output = Dropout(dropout_rate)(output)\n","  output = Dense(units=out_size, activation='softmax')(output)\n","  model = Model(inputs=tils, output=output)\n","      \n","  adam_optimizer = keras.optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n","  model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=[ 'accuracy'])\n"," \n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mY5WPqZHBR7c","colab_type":"code","outputId":"494ff6a9-b509-4316-bd22-48c9a087e934","executionInfo":{"status":"ok","timestamp":1559032999667,"user_tz":-600,"elapsed":2041,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","words_stop= stopwords.words('english')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"162FkG_CgKuj","colab_type":"code","outputId":"27e50a8e-5aac-4506-a42e-4bea922f2f89","executionInfo":{"status":"ok","timestamp":1559033000071,"user_tz":-600,"elapsed":1308,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"8NYKE8nJIbVU","colab_type":"code","colab":{}},"source":["keep_punctuation=True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCr6YWV43aDu","colab_type":"code","colab":{}},"source":["import numpy as np\n","from tqdm import tqdm\n","from string import punctuation as p\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from gensim.models import KeyedVectors\n","from keras.utils import np_utils\n","import json\n","\n","class Dataloader(object):\n","    def __init__(self,clean_data=True, remove_stopwords=True):\n","        self.q1_data, self.q2_data, self.label = self.read_dataset(\"word2train.json\")\n","        self.embedding_index = self.load_pretrain_embedding(\"model.txt\")\n","        self.word_num=len(self.embedding_index.wv.vocab)\n","        if clean_data:\n","            if remove_stopwords:\n","                self.ignored_word = words_stop\n","            self.cleaned_q1_data, self.cleaned_q2_data = [], []\n","            for text in self.q1_data:\n","                self.cleaned_q1_data.append(self.clean_data(text))\n","            for text in self.q2_data:\n","                self.cleaned_q2_data.append(self.clean_data(text))\n","        self.q1_sequences, self.q2_sequences, self.word_index = self.tokenizer()\n","        self.nb_words, self.embedding_matrix = self.prepare_embedding_matrix()\n","\n","    def read_dataset(self, train_path):\n","        q1_data=[]\n","        q2_data=[]\n","        label=[]\n","        with open(train_path,'r') as f:\n","            file_content=json.load(f)\n","\n","        for key in file_content.keys():\n","            detail_content = file_content[key]\n","            evidences=detail_content['evidence']\n","          \n","            count=0\n","\n","            for evidence in evidences:\n","              if(count>=5):\n","                break\n","              q2_data.append(evidence[2])\n","              q1_data.append(detail_content['claim'])\n","              count+=1\n","               \n","            while(count<5):\n","              q2_data.append(\"abcd\")\n","              q1_data.append(detail_content['claim'])\n","              count+=1\n","            \n","            if(detail_content['label']=='SUPPORTS'):\n","                label.append(1)\n","            elif(detail_content['label']=='REFUTES'):\n","                label.append(0)\n","            else:\n","                label.append(2)\n","\n","        label=np_utils.to_categorical(label, 3)\n","   \n","        return q1_data, q2_data, label\n","\n","    def load_pretrain_embedding(self, file):\n","        print('Indexing word vector...')\n","        embedding_index = KeyedVectors.load_word2vec_format(file,binary=False)\n","\n","        return embedding_index\n","    def clean_data(self, text):\n","        try:\n","          text = text.lower()\n","        except:\n","          print(text)\n","        words=nltk.tokenize.word_tokenize(text)\n","        text = \" \".join([word.lower() for word in words if word not in self.ignored_word and word.isalpha()])\n","\n","        return text\n","\n","    def tokenizer(self):\n","        tokenizer = Tokenizer(num_words=None)\n","        tokenizer.fit_on_texts(self.cleaned_q1_data + self.cleaned_q2_data)\n","        q1_sequences = tokenizer.texts_to_sequences(self.cleaned_q1_data)\n","        q2_sequences = tokenizer.texts_to_sequences(self.cleaned_q2_data)\n","\n","        word_index = tokenizer.word_index\n","        print('Found %s unique tokens' % len(word_index))\n","\n","        # Padding\n","        q1_data = pad_sequences(q1_sequences, maxlen=10)\n","        print('Shape of q1_data tensor: ', q1_data.shape)\n","        q2_data = pad_sequences(q2_sequences, maxlen=10)\n","        print('Shape of q2_data tensor: ', q2_data.shape)\n","        print('Shape of label tensor: ', self.label.shape)\n","\n","        return q1_data, q2_data, word_index\n","\n","    def prepare_embedding_matrix(self):\n","        nb_words = len(self.word_index)\n","        embedding_matrix = np.zeros((nb_words + 1, 300))\n","\n","        print('Creating embedding matrix ...')\n","        for word, idx in self.word_index.items():\n","            if word in self.embedding_index.wv.vocab:\n","                embedding_vector = self.embedding_index.wv[word]\n","                embedding_matrix[idx] = embedding_vector\n","\n","        return nb_words, embedding_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UphMFRxF-2Ry","colab_type":"code","colab":{}},"source":["model_path=\"emmm\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OwupVsOO2Jzq","colab_type":"code","outputId":"d5b8d2a1-00ed-48dc-ceff-7b7c83d11565","executionInfo":{"status":"ok","timestamp":1559033021462,"user_tz":-600,"elapsed":2466,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["import warnings, os\n","import tensorflow as tf\n","import numpy as np\n","from keras.models import load_model\n","warnings.filterwarnings('ignore')\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.backend.tensorflow_backend import set_session\n","from keras.models import load_model\n","\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n","set_session(tf.Session(config=config))\n","sess = tf.InteractiveSession()\n","tf.initialize_all_variables().run()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n","Instructions for updating:\n","Use `tf.global_variables_initializer` instead.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l3XTlXRaAZzG","colab_type":"code","outputId":"93ed9220-5dfa-4b34-f170-3d9c2f709c28","executionInfo":{"status":"ok","timestamp":1559033097722,"user_tz":-600,"elapsed":70941,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 130911 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k076j_01Age0","colab_type":"code","colab":{}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse -o nonempty drive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8IcciBISEhjn","colab_type":"code","outputId":"7f5d3bb7-d6ff-4fdd-b083-7bad8be0dbf8","executionInfo":{"status":"ok","timestamp":1559033102902,"user_tz":-600,"elapsed":63953,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","%cd drive/Colab Notebooks/drive"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/Colab Notebooks/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LspuZALWUvzY","colab_type":"code","outputId":"80579c1c-52e6-464b-edad-0f99f6406c2e","executionInfo":{"status":"ok","timestamp":1559033326591,"user_tz":-600,"elapsed":271634,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["data_loader = Dataloader()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Indexing word vector...\n","Found 36986 unique tokens\n","Shape of q1_data tensor:  (505265, 10)\n","Shape of q2_data tensor:  (505265, 10)\n","Shape of label tensor:  (101053, 3)\n","Creating embedding matrix ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b0aY7oHBD3pW","colab_type":"code","outputId":"ddc35aff-0486-45e9-cb9a-4bec2b3c494f","executionInfo":{"status":"ok","timestamp":1559033369943,"user_tz":-600,"elapsed":224554,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":3097}},"source":["model = get_enhanced(data_loader.nb_words + 1, 300, data_loader.embedding_matrix,\n","                           300, 300, 0.5,\n","                           10, 3)\n","print(model.summary())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","36987\n","\n","                                                                 dot_21[0][0]                     \n","__________________________________________________________________________________________________\n","multiply_14 (Multiply)          (None, 10, 600)      0           bidirectional_26[0][0]           \n","                                                                 dot_21[0][0]                     \n","__________________________________________________________________________________________________\n","subtract_15 (Subtract)          (None, 10, 600)      0           bidirectional_29[0][0]           \n","                                                                 dot_23[0][0]                     \n","__________________________________________________________________________________________________\n","multiply_15 (Multiply)          (None, 10, 600)      0           bidirectional_29[0][0]           \n","                                                                 dot_23[0][0]                     \n","__________________________________________________________________________________________________\n","subtract_16 (Subtract)          (None, 10, 600)      0           bidirectional_30[0][0]           \n","                                                                 dot_24[0][0]                     \n","__________________________________________________________________________________________________\n","multiply_16 (Multiply)          (None, 10, 600)      0           bidirectional_30[0][0]           \n","                                                                 dot_24[0][0]                     \n","__________________________________________________________________________________________________\n","subtract_17 (Subtract)          (None, 10, 600)      0           bidirectional_33[0][0]           \n","                                                                 dot_26[0][0]                     \n","__________________________________________________________________________________________________\n","multiply_17 (Multiply)          (None, 10, 600)      0           bidirectional_33[0][0]           \n","                                                                 dot_26[0][0]                     \n","__________________________________________________________________________________________________\n","subtract_18 (Subtract)          (None, 10, 600)      0           bidirectional_34[0][0]           \n","                                                                 dot_27[0][0]                     \n","__________________________________________________________________________________________________\n","multiply_18 (Multiply)          (None, 10, 600)      0           bidirectional_34[0][0]           \n","                                                                 dot_27[0][0]                     \n","__________________________________________________________________________________________________\n","subtract_19 (Subtract)          (None, 10, 600)      0           bidirectional_37[0][0]           \n","                                                                 dot_29[0][0]                     \n","__________________________________________________________________________________________________\n","multiply_19 (Multiply)          (None, 10, 600)      0           bidirectional_37[0][0]           \n","                                                                 dot_29[0][0]                     \n","__________________________________________________________________________________________________\n","subtract_20 (Subtract)          (None, 10, 600)      0           bidirectional_38[0][0]           \n","                                                                 dot_30[0][0]                     \n","__________________________________________________________________________________________________\n","multiply_20 (Multiply)          (None, 10, 600)      0           bidirectional_38[0][0]           \n","                                                                 dot_30[0][0]                     \n","__________________________________________________________________________________________________\n","concatenate_18 (Concatenate)    (None, 10, 2400)     0           bidirectional_21[0][0]           \n","                                                                 dot_17[0][0]                     \n","                                                                 subtract_11[0][0]                \n","                                                                 multiply_11[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_19 (Concatenate)    (None, 10, 2400)     0           bidirectional_22[0][0]           \n","                                                                 dot_18[0][0]                     \n","                                                                 subtract_12[0][0]                \n","                                                                 multiply_12[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_21 (Concatenate)    (None, 10, 2400)     0           bidirectional_25[0][0]           \n","                                                                 dot_20[0][0]                     \n","                                                                 subtract_13[0][0]                \n","                                                                 multiply_13[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_22 (Concatenate)    (None, 10, 2400)     0           bidirectional_26[0][0]           \n","                                                                 dot_21[0][0]                     \n","                                                                 subtract_14[0][0]                \n","                                                                 multiply_14[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_24 (Concatenate)    (None, 10, 2400)     0           bidirectional_29[0][0]           \n","                                                                 dot_23[0][0]                     \n","                                                                 subtract_15[0][0]                \n","                                                                 multiply_15[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_25 (Concatenate)    (None, 10, 2400)     0           bidirectional_30[0][0]           \n","                                                                 dot_24[0][0]                     \n","                                                                 subtract_16[0][0]                \n","                                                                 multiply_16[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_27 (Concatenate)    (None, 10, 2400)     0           bidirectional_33[0][0]           \n","                                                                 dot_26[0][0]                     \n","                                                                 subtract_17[0][0]                \n","                                                                 multiply_17[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_28 (Concatenate)    (None, 10, 2400)     0           bidirectional_34[0][0]           \n","                                                                 dot_27[0][0]                     \n","                                                                 subtract_18[0][0]                \n","                                                                 multiply_18[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_30 (Concatenate)    (None, 10, 2400)     0           bidirectional_37[0][0]           \n","                                                                 dot_29[0][0]                     \n","                                                                 subtract_19[0][0]                \n","                                                                 multiply_19[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_31 (Concatenate)    (None, 10, 2400)     0           bidirectional_38[0][0]           \n","                                                                 dot_30[0][0]                     \n","                                                                 subtract_20[0][0]                \n","                                                                 multiply_20[0][0]                \n","__________________________________________________________________________________________________\n","bidirectional_23 (Bidirectional (None, 10, 600)      6482400     concatenate_18[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional_24 (Bidirectional (None, 10, 600)      6482400     concatenate_19[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional_27 (Bidirectional (None, 10, 600)      6482400     concatenate_21[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional_28 (Bidirectional (None, 10, 600)      6482400     concatenate_22[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional_31 (Bidirectional (None, 10, 600)      6482400     concatenate_24[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional_32 (Bidirectional (None, 10, 600)      6482400     concatenate_25[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional_35 (Bidirectional (None, 10, 600)      6482400     concatenate_27[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional_36 (Bidirectional (None, 10, 600)      6482400     concatenate_28[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional_39 (Bidirectional (None, 10, 600)      6482400     concatenate_30[0][0]             \n","__________________________________________________________________________________________________\n","bidirectional_40 (Bidirectional (None, 10, 600)      6482400     concatenate_31[0][0]             \n","__________________________________________________________________________________________________\n","concatenate_20 (Concatenate)    (None, 10, 1200)     0           bidirectional_23[0][0]           \n","                                                                 bidirectional_24[0][0]           \n","__________________________________________________________________________________________________\n","concatenate_23 (Concatenate)    (None, 10, 1200)     0           bidirectional_27[0][0]           \n","                                                                 bidirectional_28[0][0]           \n","__________________________________________________________________________________________________\n","concatenate_26 (Concatenate)    (None, 10, 1200)     0           bidirectional_31[0][0]           \n","                                                                 bidirectional_32[0][0]           \n","__________________________________________________________________________________________________\n","concatenate_29 (Concatenate)    (None, 10, 1200)     0           bidirectional_35[0][0]           \n","                                                                 bidirectional_36[0][0]           \n","__________________________________________________________________________________________________\n","concatenate_32 (Concatenate)    (None, 10, 1200)     0           bidirectional_39[0][0]           \n","                                                                 bidirectional_40[0][0]           \n","__________________________________________________________________________________________________\n","batch_normalization_19 (BatchNo (None, 10, 1200)     4800        concatenate_20[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_22 (BatchNo (None, 10, 1200)     4800        concatenate_23[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 10, 1200)     4800        concatenate_26[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 10, 1200)     4800        concatenate_29[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_31 (BatchNo (None, 10, 1200)     4800        concatenate_32[0][0]             \n","__________________________________________________________________________________________________\n","concatenate_33 (Concatenate)    (None, 10, 6000)     0           batch_normalization_19[0][0]     \n","                                                                 batch_normalization_22[0][0]     \n","                                                                 batch_normalization_25[0][0]     \n","                                                                 batch_normalization_28[0][0]     \n","                                                                 batch_normalization_31[0][0]     \n","__________________________________________________________________________________________________\n","global_average_pooling1d_2 (Glo (None, 6000)         0           concatenate_33[0][0]             \n","__________________________________________________________________________________________________\n","global_max_pooling1d_2 (GlobalM (None, 6000)         0           concatenate_33[0][0]             \n","__________________________________________________________________________________________________\n","concatenate_34 (Concatenate)    (None, 12000)        0           global_average_pooling1d_2[0][0] \n","                                                                 global_max_pooling1d_2[0][0]     \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 300)          3600300     concatenate_34[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_32 (BatchNo (None, 300)          1200        dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 300)          0           batch_normalization_32[0][0]     \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 3)            903         dropout_2[0][0]                  \n","==================================================================================================\n","Total params: 138,366,903\n","Trainable params: 82,867,803\n","Non-trainable params: 55,499,100\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p23_Pw6vYo29","colab_type":"code","colab":{}},"source":["def train_x(q1, q2, y, fold_id, batch_size, model):\n","    train_set=[[] for i in range(10)]\n","    for i in range(0,len(q1),5):\n","      for j in range(0,5):\n","        train_set[2*j].append(q1[i+j]) \n","        train_set[2*j+1].append(q2[i+j])\n","        \n","    train_y =y\n","    val_set=[[] for i in range(10)]\n","    for i in range(0,300,5):\n","      for j in range(0,5):\n","        val_set[2*j].append(q1[i+j]) \n","        val_set[2*j+1].append(q2[i+j])\n","    val_y=y[0:60]\n","    \n","    early_stopping = EarlyStopping(monitor='val_loss', patience=7)   \n","    best_model_path = model_path + 'ESIM_' + str(fold_id) + '.h5'\n","    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n","    model.fit(train_set, train_y, validation_data=(val_set, val_y),\n","                     epochs=4, batch_size=batch_size, shuffle=True,\n","                     callbacks=[early_stopping, model_checkpoint])\n","    model.save_weights(best_model_path)\n"," \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIIVVszvCIzm","colab_type":"code","outputId":"3a02f563-6659-49c8-c234-ea3220ed175b","executionInfo":{"status":"ok","timestamp":1559034732028,"user_tz":-600,"elapsed":295385,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":577}},"source":["\n","\n","models = train_x(data_loader.q1_sequences,\n","                 data_loader.q2_sequences,\n","                 data_loader.label,\n","                 2,\n","                 1024,\n","                 model)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","Train on 101053 samples, validate on 60 samples\n","Epoch 1/4\n","Train on 101053 samples, validate on 60 samples\n","Epoch 1/4\n","101053/101053 [==============================] - 259s 3ms/step - loss: 0.3660 - acc: 0.8269 - val_loss: 0.2418 - val_acc: 0.8778\n","101053/101053 [==============================] - 259s 3ms/step - loss: 0.3660 - acc: 0.8269 - val_loss: 0.2418 - val_acc: 0.8778\n","Epoch 2/4\n","Epoch 2/4\n","101053/101053 [==============================] - 251s 2ms/step - loss: 0.2710 - acc: 0.8571 - val_loss: 0.1774 - val_acc: 0.8667\n","101053/101053 [==============================] - 251s 2ms/step - loss: 0.2710 - acc: 0.8571 - val_loss: 0.1774 - val_acc: 0.8667\n","Epoch 3/4\n","Epoch 3/4\n","101053/101053 [==============================] - 252s 2ms/step - loss: 0.2447 - acc: 0.8696 - val_loss: 0.1544 - val_acc: 0.9111\n","101053/101053 [==============================] - 252s 2ms/step - loss: 0.2447 - acc: 0.8696 - val_loss: 0.1544 - val_acc: 0.9111\n","Epoch 4/4\n","Epoch 4/4\n","101053/101053 [==============================] - 248s 2ms/step - loss: 0.2272 - acc: 0.8808 - val_loss: 0.1386 - val_acc: 0.9111\n","101053/101053 [==============================] - 248s 2ms/step - loss: 0.2272 - acc: 0.8808 - val_loss: 0.1386 - val_acc: 0.9111\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h7hY2kLzaSjZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWY5z9s-jucp","colab_type":"code","colab":{}},"source":["class Testloader(object):\n","    def __init__(self,clean_data=True, remove_stopwords=True):\n","        self.q1_data, self.q2_data, self.label = self.read_dataset(\"test-unlabelled_evidence_0.8_siameseLSTM_epoch9_with_sentence.json\")\n","        self.embedding_index = self.load_pretrain_embedding(\"model.txt\")\n","        self.word_num=len(self.embedding_index.wv.vocab)\n","        if clean_data:\n","            if remove_stopwords:\n","                self.ignored_word = words_stop\n","            self.cleaned_q1_data, self.cleaned_q2_data = [], []\n","            for text in self.q1_data:\n","                self.cleaned_q1_data.append(self.clean_data(text))\n","            for text in self.q2_data:\n","                self.cleaned_q2_data.append(self.clean_data(text))\n","        self.q1_sequences, self.q2_sequences, self.word_index = self.tokenizer()\n","\n","    def read_dataset(self, train_path):\n","        q1_data=[]\n","        q2_data=[]\n","        label=[]\n","        with open(train_path,'r') as f:\n","            file_content=json.load(f)\n","\n","        for key in file_content.keys():\n","            detail_content = file_content[key]\n","            evidences=detail_content['evidence']\n","          \n","            count=0\n","\n","            for evidence in evidences:\n","              if(count>=5):\n","                break\n","              q2_data.append(evidence[2])\n","              q1_data.append(detail_content['claim'])\n","              count+=1\n","               \n","            while(count<5):\n","              q2_data.append(\"abcd\")\n","              q1_data.append(detail_content['claim'])\n","              count+=1\n","            \n","            if(detail_content['label']=='SUPPORTS'):\n","                label.append(1)\n","            elif(detail_content['label']=='REFUTES'):\n","                label.append(0)\n","            else:\n","                label.append(2)\n","\n","        label=np_utils.to_categorical(label, 3)\n","   \n","        return q1_data, q2_data, label\n","\n","    def load_pretrain_embedding(self, file):\n","        print('Indexing word vector...')\n","        embedding_index = KeyedVectors.load_word2vec_format(file,binary=False)\n","\n","        return embedding_index\n","    def clean_data(self, text):\n","        try:\n","          text = text.lower()\n","        except:\n","          print(text)\n","        words=nltk.tokenize.word_tokenize(text)\n","        text = \" \".join([word.lower() for word in words if word not in self.ignored_word and word.isalpha()])\n","\n","        return text\n","\n","    def tokenizer(self):\n","        tokenizer = Tokenizer(num_words=None)\n","        tokenizer.fit_on_texts(self.cleaned_q1_data + self.cleaned_q2_data)\n","        q1_sequences = tokenizer.texts_to_sequences(self.cleaned_q1_data)\n","        q2_sequences = tokenizer.texts_to_sequences(self.cleaned_q2_data)\n","\n","        word_index = tokenizer.word_index\n","        print('Found %s unique tokens' % len(word_index))\n","\n","        # Padding\n","        q1_data = pad_sequences(q1_sequences, maxlen=10)\n","        print('Shape of q1_data tensor: ', q1_data.shape)\n","        q2_data = pad_sequences(q2_sequences, maxlen=10)\n","        print('Shape of q2_data tensor: ', q2_data.shape)\n","        print('Shape of label tensor: ', self.label.shape)\n","\n","        return q1_data, q2_data, word_index"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sAf7uUz9nuqy","colab_type":"code","outputId":"29ee1519-2628-4621-83d9-f4749f85f7d5","executionInfo":{"status":"ok","timestamp":1559034851711,"user_tz":-600,"elapsed":117497,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["test_loader=Testloader()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Indexing word vector...\n","Found 27726 unique tokens\n","Shape of q1_data tensor:  (74985, 10)\n","Shape of q2_data tensor:  (74985, 10)\n","Shape of label tensor:  (14997, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QK8gzg9P0S4u","colab_type":"code","colab":{}},"source":["print(test_loader.cleaned_q1_data[1:1000])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9B1zPIqn0V3Y","colab_type":"code","colab":{}},"source":["print(test_loader.cleaned_q2_data[1:1000])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3AjLrSsnxQJ","colab_type":"code","outputId":"cb758c2e-1299-4ff9-8662-65dc17418f6f","executionInfo":{"status":"ok","timestamp":1559034910014,"user_tz":-600,"elapsed":170311,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["import warnings, os\n","from keras.models import load_model\n","warnings.filterwarnings('ignore')\n","\n","model_path='emmmESIM_2.h5'\n","model = get_enhanced(data_loader.nb_words+1, 300, data_loader.embedding_matrix,\n","                           300, 300, 0.5,\n","                           10, 3)\n","\n","model.load_weights(model_path)\n","\n","q1=test_loader.q1_sequences\n","q2=test_loader.q2_sequences\n","test_set=[[] for i in range(10)]\n","for i in range(0,len(test_loader.q1_sequences),5):\n","    for j in range(0,5):\n","      test_set[2*j].append(q1[i+j]) \n","      test_set[2*j+1].append(q2[i+j])\n","eval_predict = model.predict(test_set, batch_size=1024, verbose=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["36987\n","14997/14997 [==============================] - 13s 882us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tyheGu3rnztE","colab_type":"code","colab":{}},"source":["\n","\n","with open(\"test-unlabelled_evidence_0.8_siameseLSTM_epoch9_with_sentence.json\",'r') as f:\n","      file_content=json.load(f)\n","      for i,key in enumerate(file_content.keys()):\n","          detail_content = file_content[key]\n","          result=eval_predict[i]\n","          lab=np.argmax(result)\n","          if(lab==1):\n","              detail_content['label']='SUPPORTS'\n","          elif(lab==0):\n","              detail_content['label']='REFUTES'\n","          else:\n","              detail_content['label']='NOT ENOUGH INFO'\n","\n","          evidences=detail_content['evidence']\n","          if(evidences==[]):\n","            detail_content['label']='NOT ENOUGH INFO'\n","          for evidence in evidences:\n","            del evidence[2]\n","\n","with open('word2vec-result2.json','w') as t:\n","      json.dump(file_content, t, indent=2, separators=(',',':'))"],"execution_count":0,"outputs":[]}]}