{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"esim.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YWgjjJNOA0Tk","colab_type":"text"},"source":["#this file does pure ESIM model \n","including training and predicting\n","\n","written by Linghan Zhou\n","\n","---\n"]},{"cell_type":"code","metadata":{"id":"AdmlOSNY2CJH","colab_type":"code","colab":{}},"source":["import keras\n","from keras.layers import *\n","from keras.activations import softmax\n","from keras.models import Model\n","from keras.layers.merge import concatenate\n","from keras.layers.normalization import BatchNormalization\n","\n","\n","def get_ESIM_model(nb_words, embedding_dim, embedding_matrix, recurrent_units, dense_units, dropout_rate, max_sequence_length, out_size):\n","    embedding_layer = Embedding(nb_words,\n","                                embedding_dim,\n","                                weights=[embedding_matrix],\n","                                input_length=max_sequence_length,\n","                                trainable=False)\n","\n","    input_q1_layer = Input(shape=(max_sequence_length,), dtype='int32', name='q1')\n","    input_q2_layer = Input(shape=(max_sequence_length,), dtype='int32', name='q2')\n","    \n","    embedding_sequence_q1 = BatchNormalization(axis=2)(embedding_layer(input_q1_layer))\n","    embedding_sequence_q2 = BatchNormalization(axis=2)(embedding_layer(input_q2_layer))\n","\n","    final_embedding_sequence_q1 = SpatialDropout1D(0.25)(embedding_sequence_q1)\n","    final_embedding_sequence_q2 = SpatialDropout1D(0.25)(embedding_sequence_q2)\n","\n","    rnn_layer_q1 = Bidirectional(LSTM(recurrent_units, return_sequences=True))(final_embedding_sequence_q1)\n","    rnn_layer_q2 = Bidirectional(LSTM(recurrent_units, return_sequences=True))(final_embedding_sequence_q2)\n","\n","    attention = Dot(axes=-1)([rnn_layer_q1, rnn_layer_q2])\n","    w_attn_1 = Lambda(lambda x: softmax(x, axis=1))(attention)\n","    w_attn_2 = Permute((2, 1))(Lambda(lambda x: softmax(x, axis=2))(attention))\n","    align_layer_1 = Dot(axes=1)([w_attn_1, rnn_layer_q1])\n","    align_layer_2 = Dot(axes=1)([w_attn_2, rnn_layer_q2])\n","\n","    subtract_layer_1 = subtract([rnn_layer_q1, align_layer_1])\n","    subtract_layer_2 = subtract([rnn_layer_q2, align_layer_2])\n","\n","    multiply_layer_1 = multiply([rnn_layer_q1, align_layer_1])\n","    multiply_layer_2 = multiply([rnn_layer_q2, align_layer_2])\n","\n","    m_q1 = concatenate([rnn_layer_q1, align_layer_1, subtract_layer_1, multiply_layer_1])\n","    m_q2 = concatenate([rnn_layer_q2, align_layer_2, subtract_layer_2, multiply_layer_2])\n","\n","    v_q1_i = Bidirectional(LSTM(recurrent_units, return_sequences=True))(m_q1)\n","    v_q2_i = Bidirectional(LSTM(recurrent_units, return_sequences=True))(m_q2)\n","\n","    avgpool_q1 = GlobalAveragePooling1D()(v_q1_i)\n","    avgpool_q2 = GlobalAveragePooling1D()(v_q2_i)\n","    maxpool_q1 = GlobalMaxPooling1D()(v_q1_i)\n","    maxpool_q2 = GlobalMaxPooling1D()(v_q2_i)\n","\n","    merged_q1 = concatenate([avgpool_q1, maxpool_q1])\n","    merged_q2 = concatenate([avgpool_q2, maxpool_q2])\n","\n","    final_v = BatchNormalization()(concatenate([merged_q1, merged_q2]))\n","    output = Dense(units=dense_units, activation='relu')(final_v)\n","    output = BatchNormalization()(output)\n","    output = Dropout(dropout_rate)(output)\n","    output = Dense(units=out_size, activation='softmax')(output)\n","\n","    model = Model(inputs=[input_q1_layer, input_q2_layer], output=output)\n","    adam_optimizer = keras.optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n","\n","    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=[ 'binary_crossentropy', 'accuracy'])\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mY5WPqZHBR7c","colab_type":"code","outputId":"657c8832-7cbf-44af-f628-325ffd9a45ea","executionInfo":{"status":"ok","timestamp":1559123720163,"user_tz":-600,"elapsed":2682,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","words_stop= stopwords.words('english')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"162FkG_CgKuj","colab_type":"code","outputId":"52df34e2-2b4a-4977-f270-11d85bb0ee24","executionInfo":{"status":"ok","timestamp":1559123720638,"user_tz":-600,"elapsed":1302,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["nltk.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"8NYKE8nJIbVU","colab_type":"code","colab":{}},"source":["keep_punctuation=True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCr6YWV43aDu","colab_type":"code","colab":{}},"source":["import numpy as np\n","from tqdm import tqdm\n","from string import punctuation as p\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from gensim.models import KeyedVectors\n","from keras.utils import np_utils\n","import json\n","\n","class Dataloader(object):\n","    def __init__(self,clean_data=True, remove_stopwords=True):\n","        self.q1_data, self.q2_data, self.label = self.read_dataset(\"word2train.json\")\n","        self.embedding_index = self.load_pretrain_embedding(\"model.txt\")\n","        self.word_num=len(self.embedding_index.wv.vocab)\n","        if clean_data:\n","            if remove_stopwords:\n","                self.ignored_word = words_stop\n","            self.cleaned_q1_data, self.cleaned_q2_data = [], []\n","            for text in self.q1_data:\n","                self.cleaned_q1_data.append(self.clean_data(text))\n","            for text in self.q2_data:\n","                self.cleaned_q2_data.append(self.clean_data(text))\n","        self.q1_sequences, self.q2_sequences, self.word_index = self.tokenizer()\n","        self.nb_words, self.embedding_matrix = self.prepare_embedding_matrix()\n","\n","    def read_dataset(self, train_path):\n","        q1_data=[]\n","        q2_data=[]\n","        label=[]\n","        with open(train_path,'r') as f:\n","            file_content=json.load(f)\n","\n","        for key in file_content.keys():\n","            detail_content = file_content[key]\n","            evidences=detail_content['evidence']\n","          \n","            temp_evidence=\" \"\n","            for evidence in evidences:\n","              temp_evidence=temp_evidence+evidence[2]\n"," \n","            q1_data.append(detail_content['claim'])\n","            q2_data.append(temp_evidence)\n","    \n","            if(detail_content['label']=='SUPPORTS'):\n","                label.append(1)\n","            elif(detail_content['label']=='REFUTES'):\n","                label.append(0)\n","            else:\n","                label.append(2)\n","\n","        label=np_utils.to_categorical(label, 3)\n","   \n","        return q1_data, q2_data, label\n","\n","    def load_pretrain_embedding(self, file):\n","        print('Indexing word vector...')\n","        embedding_index = KeyedVectors.load_word2vec_format(file,binary=False)\n","\n","        return embedding_index\n","    def clean_data(self, text):\n","        try:\n","          text = text.lower()\n","        except:\n","          print(text)\n","        words=nltk.tokenize.word_tokenize(text)\n","        text = \" \".join([word.lower() for word in words if word not in self.ignored_word and word.isalpha()])\n","\n","        return text\n","\n","    def tokenizer(self):\n","        tokenizer = Tokenizer(num_words=None)\n","        tokenizer.fit_on_texts(self.cleaned_q1_data + self.cleaned_q2_data)\n","        q1_sequences = tokenizer.texts_to_sequences(self.cleaned_q1_data)\n","        q2_sequences = tokenizer.texts_to_sequences(self.cleaned_q2_data)\n","\n","        word_index = tokenizer.word_index\n","        print('Found %s unique tokens' % len(word_index))\n","\n","        # Padding\n","        q1_data = pad_sequences(q1_sequences, maxlen=50)\n","        print('Shape of q1_data tensor: ', q1_data.shape)\n","        q2_data = pad_sequences(q2_sequences, maxlen=50)\n","        print('Shape of q2_data tensor: ', q2_data.shape)\n","        print('Shape of label tensor: ', self.label.shape)\n","\n","        return q1_data, q2_data, word_index\n","\n","    def prepare_embedding_matrix(self):\n","        nb_words = len(self.word_index)\n","        embedding_matrix = np.zeros((nb_words + 1, 300))\n","\n","        print('Creating embedding matrix ...')\n","        for word, idx in self.word_index.items():\n","            if word in self.embedding_index.wv.vocab:\n","                embedding_vector = self.embedding_index.wv[word]\n","                embedding_matrix[idx] = embedding_vector\n","\n","        return nb_words, embedding_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UphMFRxF-2Ry","colab_type":"code","colab":{}},"source":["model_path=\"esm3\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OwupVsOO2Jzq","colab_type":"code","colab":{}},"source":["import warnings, os\n","import tensorflow as tf\n","import numpy as np\n","from sklearn.metrics import accuracy_score, log_loss, precision_score, recall_score, f1_score\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.backend.tensorflow_backend import set_session\n","from keras.models import load_model\n","warnings.filterwarnings('ignore')\n","\n","\n","# Init settings\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n","set_session(tf.Session(config=config))\n","\n","\n","def train_model_by_logloss(model, batch_size, train_q1, train_q2, train_y, val_q1, val_q2, val_y, fold_id):\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n","    best_model_path = model_path + 'ESIM_' + str(fold_id) + '.h5'\n","    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n","    hist = model.fit([train_q1, train_q2], train_y, validation_data=([val_q1, val_q2], val_y),\n","                     epochs=2, batch_size=batch_size, shuffle=True,\n","                     callbacks=[early_stopping, model_checkpoint])\n","    best_val_score = min(hist.history['val_loss'])\n","    predictions = model.predict([val_q1, val_q2])\n","\n","    return model, best_val_score, predictions\n","\n","def train_folds(q1, q2, y, fold_count, batch_size, get_model_func):\n","    fold_size = len(q1) // fold_count\n","    models, fold_predictions = [], []\n","    score, total_auc = 0, 0\n","    write_file = open('Logger.txt', 'w', encoding='utf-8')\n","    for fold_id in range(0, fold_count):\n","        fold_start = fold_size * fold_id\n","        fold_end = fold_start + fold_size\n","\n","        if fold_id == fold_count - 1:\n","            fold_end = len(q1)\n","\n","        train_q1 = np.concatenate([q1[:fold_start], q1[fold_end:]])\n","        train_q2 = np.concatenate([q2[:fold_start], q2[fold_end:]])\n","        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n","\n","        val_q1 = q1[fold_start: fold_end]\n","        val_q2 = q2[fold_start: fold_end]\n","        val_y = y[fold_start: fold_end]\n","\n","        print('In fold {}'.format(fold_id + 1))\n","        model, best_val_score, fold_prediction = train_model_by_logloss(get_model_func, batch_size,\n","                                                                             train_q1, train_q2, train_y,\n","                                                                             val_q1, val_q2, val_y, fold_id)\n","        score += best_val_score\n","        fold_predictions.append(fold_prediction)\n","        models.append(model)\n","        write_file.write('Fold {}\\tLoss {}\\n'.format(fold_id + 1, best_val_score))\n","        write_file.flush()\n","    \n","    write_file.close()\n","\n","    return models, score / fold_count,  fold_predictions\n","\n","\n","def train():\n","    # q1 & q2 sequences (after tokenize operation) + label + embedding_matrix\n","    #data_loader = Dataloader()\n","    if not os.path.exists(model_path):\n","        os.makedirs(model_path)\n","\n","    model = get_ESIM_model(data_loader.nb_words + 1, 300, data_loader.embedding_matrix,\n","                           300, 300, 0.5,\n","                           50, 3)\n","    # model = get_ESIM_model(pm.MAX_NB_WORDS, pm.EMBEDDING_DIM, None,\n","    #                        pm.RECURRENT_UNITS, pm.DENSE_UNITS, pm.DROPOUT_RATE,\n","    #                        pm.MAX_SEQUENCE_LENGTH, 1)\n","    print(model.summary())\n","\n","    models, val_loss,  fold_predictions = train_folds(data_loader.q1_sequences,\n","                                                                data_loader.q2_sequences,\n","                                                                data_loader.label,\n","                                                                2,\n","                                                                1024,\n","                                                                model)\n","\n","    print('Overall val-loss: {}'.format(val_loss))\n","\n","\n","def evaluate():\n","    '''\n","    For training OOB(out-of-bag) Evaluation.\n","    '''\n","    data_loader = Dataloader()\n","    eval_predicts_list = []\n","    for fold_id in range(0, 10):\n","        model = get_ESIM_model(data_loader.nb_words + 1, 300, data_loader.embedding_matrix,\n","                           300, 300, 0.5, 50, 3)\n","\n","        model.load_weights(model_path + 'ESIM_' + str(fold_id) + '.h5')\n","        eval_predict = model.predict([data_loader.q1_sequences, data_loader.q2_sequences], \n","                                     batch_size=1024, verbose=1)\n","        eval_predicts_list.append(eval_predict)\n","    \n","        train_loss = log_loss(data_loader.label, eval_predict)\n","        train_acc = accuracy_score(data_loader.label, eval_predict.round())\n","        train_precision = precision_score(data_loader.label, eval_predict.round())\n","        train_recall = recall_score(data_loader.label, eval_predict.round())\n","        train_f1_score = f1_score(data_loader.label, eval_predict.round())\n","        print('Training LOSS:{}\\tACCURACY:{}\\tPRECISION:{}\\tRECALL:{}\\tF1_SCORE:{}'.format(\n","             train_loss, train_acc, train_precision, train_recall, train_f1_score))\n","\n","    \n","    train_fold_predictions = np.zeros(eval_predicts_list[0].shape)\n","    for fold_predict in eval_predicts_list:\n","        train_fold_predictions += fold_predict\n","    train_fold_predictions /= len(eval_predicts_list)\n","\n","    train_auc = roc_auc_score(data_loader.label, train_fold_predictions)\n","    train_loss = log_loss(data_loader.label, train_fold_predictions)\n","    train_acc = accuracy_score(data_loader.label, train_fold_predictions.round())\n","    train_precision = precision_score(data_loader.label, train_fold_predictions.round())\n","    train_recall = recall_score(data_loader.label, train_fold_predictions.round())\n","    train_f1_score = f1_score(data_loader.label, train_fold_predictions.round())\n","    print('Training LOSS:{}\\tACCURACY:{}\\tPRECISION:{}\\tRECALL:{}\\tF1_SCORE:{}'.format(\n","         train_loss, train_acc, train_precision, train_recall, train_f1_score))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWY5z9s-jucp","colab_type":"code","colab":{}},"source":["class Testloader(object):\n","    def __init__(self,clean_data=True, remove_stopwords=True):\n","        self.q1_data, self.q2_data, self.label = self.read_dataset(\"test-unlabelled_evidence_0.8_siameseLSTM_epoch9_with_sentence.json\")\n","        if clean_data:\n","            if remove_stopwords:\n","                self.ignored_word = words_stop\n","            self.cleaned_q1_data, self.cleaned_q2_data = [], []\n","            for text in self.q1_data:\n","                self.cleaned_q1_data.append(self.clean_data(text))\n","            for text in self.q2_data:\n","                self.cleaned_q2_data.append(self.clean_data(text))\n","        self.q1_sequences, self.q2_sequences, self.word_index = self.tokenizer()\n","       \n","    def read_dataset(self, train_path):\n","        q1_data=[]\n","        q2_data=[]\n","        label=[]\n","        with open(train_path,'r') as f:\n","            file_content=json.load(f)\n","\n","        for key in file_content.keys():\n","            detail_content = file_content[key]\n","            q1_data.append (detail_content['claim'])\n","\n","            evidences=detail_content['evidence']\n","            temp_evidence=\"\"\n","            for evidence in evidences:\n","              temp_evidence+=evidence[2]\n","            \n","            q2_data.append(temp_evidence)\n","\n","        label = np.array(label)    \n","        return q1_data, q2_data, label\n","\n","    def clean_data(self, text):\n","        try:\n","          text = text.lower()\n","        except:\n","          print(text)\n","       \n","        words=nltk.tokenize.word_tokenize(text)\n","        text = \" \".join([word.lower() for word in words if word not in self.ignored_word and word.isalpha()])\n","\n","        return text\n","\n","    def tokenizer(self):\n","        tokenizer = Tokenizer(num_words=None)\n","        tokenizer.fit_on_texts(self.cleaned_q1_data + self.cleaned_q2_data)\n","        q1_sequences = tokenizer.texts_to_sequences(self.cleaned_q1_data)\n","        q2_sequences = tokenizer.texts_to_sequences(self.cleaned_q2_data)\n","\n","        word_index = tokenizer.word_index\n","        print('Found %s unique tokens' % len(word_index))\n","\n","        # Padding\n","        q1_data = pad_sequences(q1_sequences, maxlen=50)\n","        print('Shape of q1_data tensor: ', q1_data.shape)\n","        q2_data = pad_sequences(q2_sequences, maxlen=50)\n","        print('Shape of q2_data tensor: ', q2_data.shape)\n","        print('Shape of label tensor: ', self.label.shape)\n","\n","        return q1_data, q2_data, word_index\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l3XTlXRaAZzG","colab_type":"code","outputId":"85e08a5f-72b3-418d-b726-936edd3a52b4","executionInfo":{"status":"ok","timestamp":1559124055501,"user_tz":-600,"elapsed":172630,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":11,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k076j_01Age0","colab_type":"code","colab":{}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse -o nonempty drive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8IcciBISEhjn","colab_type":"code","outputId":"e280a978-d676-4f79-9916-4fc22a267633","executionInfo":{"status":"ok","timestamp":1559124066454,"user_tz":-600,"elapsed":8210,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","%cd drive/Colab Notebooks/drive"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/content/drive/Colab Notebooks/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LspuZALWUvzY","colab_type":"code","outputId":"76c61974-cf94-4150-8c85-448c80b92d33","executionInfo":{"status":"ok","timestamp":1559124315288,"user_tz":-600,"elapsed":254755,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["data_loader = Dataloader()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Indexing word vector...\n","Found 39689 unique tokens\n","Shape of q1_data tensor:  (101053, 50)\n","Shape of q2_data tensor:  (101053, 50)\n","Shape of label tensor:  (101053, 3)\n","Creating embedding matrix ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PmeGtOmK2kww","colab_type":"code","outputId":"d68018f7-d26b-4909-f6c5-fe1fe24211e3","executionInfo":{"status":"error","timestamp":1559125453374,"user_tz":-600,"elapsed":894802,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":2341}},"source":["train()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","q1 (InputLayer)                 (None, 50)           0                                            \n","__________________________________________________________________________________________________\n","q2 (InputLayer)                 (None, 50)           0                                            \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 50, 300)      11907000    q1[0][0]                         \n","                                                                 q2[0][0]                         \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 50, 300)      1200        embedding_2[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 50, 300)      1200        embedding_2[1][0]                \n","__________________________________________________________________________________________________\n","spatial_dropout1d_3 (SpatialDro (None, 50, 300)      0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","spatial_dropout1d_4 (SpatialDro (None, 50, 300)      0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","bidirectional_5 (Bidirectional) (None, 50, 600)      1442400     spatial_dropout1d_3[0][0]        \n","__________________________________________________________________________________________________\n","bidirectional_6 (Bidirectional) (None, 50, 600)      1442400     spatial_dropout1d_4[0][0]        \n","__________________________________________________________________________________________________\n","dot_4 (Dot)                     (None, 50, 50)       0           bidirectional_5[0][0]            \n","                                                                 bidirectional_6[0][0]            \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 50, 50)       0           dot_4[0][0]                      \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 50, 50)       0           dot_4[0][0]                      \n","__________________________________________________________________________________________________\n","permute_2 (Permute)             (None, 50, 50)       0           lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","dot_5 (Dot)                     (None, 50, 600)      0           lambda_3[0][0]                   \n","                                                                 bidirectional_5[0][0]            \n","__________________________________________________________________________________________________\n","dot_6 (Dot)                     (None, 50, 600)      0           permute_2[0][0]                  \n","                                                                 bidirectional_6[0][0]            \n","__________________________________________________________________________________________________\n","subtract_3 (Subtract)           (None, 50, 600)      0           bidirectional_5[0][0]            \n","                                                                 dot_5[0][0]                      \n","__________________________________________________________________________________________________\n","multiply_3 (Multiply)           (None, 50, 600)      0           bidirectional_5[0][0]            \n","                                                                 dot_5[0][0]                      \n","__________________________________________________________________________________________________\n","subtract_4 (Subtract)           (None, 50, 600)      0           bidirectional_6[0][0]            \n","                                                                 dot_6[0][0]                      \n","__________________________________________________________________________________________________\n","multiply_4 (Multiply)           (None, 50, 600)      0           bidirectional_6[0][0]            \n","                                                                 dot_6[0][0]                      \n","__________________________________________________________________________________________________\n","concatenate_6 (Concatenate)     (None, 50, 2400)     0           bidirectional_5[0][0]            \n","                                                                 dot_5[0][0]                      \n","                                                                 subtract_3[0][0]                 \n","                                                                 multiply_3[0][0]                 \n","__________________________________________________________________________________________________\n","concatenate_7 (Concatenate)     (None, 50, 2400)     0           bidirectional_6[0][0]            \n","                                                                 dot_6[0][0]                      \n","                                                                 subtract_4[0][0]                 \n","                                                                 multiply_4[0][0]                 \n","__________________________________________________________________________________________________\n","bidirectional_7 (Bidirectional) (None, 50, 600)      6482400     concatenate_6[0][0]              \n","__________________________________________________________________________________________________\n","bidirectional_8 (Bidirectional) (None, 50, 600)      6482400     concatenate_7[0][0]              \n","__________________________________________________________________________________________________\n","global_average_pooling1d_3 (Glo (None, 600)          0           bidirectional_7[0][0]            \n","__________________________________________________________________________________________________\n","global_max_pooling1d_3 (GlobalM (None, 600)          0           bidirectional_7[0][0]            \n","__________________________________________________________________________________________________\n","global_average_pooling1d_4 (Glo (None, 600)          0           bidirectional_8[0][0]            \n","__________________________________________________________________________________________________\n","global_max_pooling1d_4 (GlobalM (None, 600)          0           bidirectional_8[0][0]            \n","__________________________________________________________________________________________________\n","concatenate_8 (Concatenate)     (None, 1200)         0           global_average_pooling1d_3[0][0] \n","                                                                 global_max_pooling1d_3[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_9 (Concatenate)     (None, 1200)         0           global_average_pooling1d_4[0][0] \n","                                                                 global_max_pooling1d_4[0][0]     \n","__________________________________________________________________________________________________\n","concatenate_10 (Concatenate)    (None, 2400)         0           concatenate_8[0][0]              \n","                                                                 concatenate_9[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 2400)         9600        concatenate_10[0][0]             \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 300)          720300      batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 300)          1200        dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 300)          0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 3)            903         dropout_2[0][0]                  \n","==================================================================================================\n","Total params: 28,491,003\n","Trainable params: 16,577,403\n","Non-trainable params: 11,913,600\n","__________________________________________________________________________________________________\n","None\n","In fold 1\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n","Train on 50527 samples, validate on 50526 samples\n","Epoch 1/2\n","50527/50527 [==============================] - 165s 3ms/step - loss: 0.3832 - binary_crossentropy: 0.3832 - acc: 0.8691 - val_loss: 0.7696 - val_binary_crossentropy: 0.7696 - val_acc: 0.7327\n","Epoch 2/2\n","50527/50527 [==============================] - 159s 3ms/step - loss: 0.2485 - binary_crossentropy: 0.2485 - acc: 0.9120 - val_loss: 0.3629 - val_binary_crossentropy: 0.3629 - val_acc: 0.7749\n","In fold 2\n","Train on 50526 samples, validate on 50527 samples\n","Epoch 1/2\n","50176/50526 [============================>.] - ETA: 0s - loss: 0.0441 - binary_crossentropy: 0.0441 - acc: 0.9879"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-691877040d65>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m                                                                 \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                                                                 \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                                                                 model)\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Overall val-loss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-691877040d65>\u001b[0m in \u001b[0;36mtrain_folds\u001b[0;34m(q1, q2, y, fold_count, batch_size, get_model_func)\u001b[0m\n\u001b[1;32m     51\u001b[0m         model, best_val_score, fold_prediction = train_model_by_logloss(get_model_func, batch_size,\n\u001b[1;32m     52\u001b[0m                                                                              \u001b[0mtrain_q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_q2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                                                                              val_q1, val_q2, val_y, fold_id)\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbest_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mfold_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-691877040d65>\u001b[0m in \u001b[0;36mtrain_model_by_logloss\u001b[0;34m(model, batch_size, train_q1, train_q2, train_y, val_q1, val_q2, val_y, fold_id)\u001b[0m\n\u001b[1;32m     22\u001b[0m     hist = model.fit([train_q1, train_q2], train_y, validation_data=([val_q1, val_q2], val_y),\n\u001b[1;32m     23\u001b[0m                      \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                      callbacks=[early_stopping, model_checkpoint])\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mbest_val_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_q2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"sAf7uUz9nuqy","colab_type":"code","outputId":"42498a47-9436-48e5-fb07-030482d6f7fc","executionInfo":{"status":"ok","timestamp":1558996926085,"user_tz":-600,"elapsed":16276,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["test_loader=Testloader()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 27726 unique tokens\n","Shape of q1_data tensor:  (14997, 50)\n","Shape of q2_data tensor:  (14997, 50)\n","Shape of label tensor:  (0,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w3AjLrSsnxQJ","colab_type":"code","outputId":"b23b382d-5e48-460b-ae5f-e115b971b829","executionInfo":{"status":"ok","timestamp":1559000068903,"user_tz":-600,"elapsed":30592,"user":{"displayName":"ZHOU LINGHAN","photoUrl":"","userId":"08289592578283471149"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import warnings, os\n","from keras.models import load_model\n","warnings.filterwarnings('ignore')\n","\n","model_path='esm3ESIM_1.h5ESIM_1.h5'\n","model = get_ESIM_model(data_loader.nb_words+1, 300, data_loader.embedding_matrix,\n","                           300, 300, 0.5, 50, 3)\n","\n","model.load_weights(model_path)\n","eval_predict = model.predict([test_loader.q1_sequences, test_loader.q2_sequences], \n","                                     batch_size=1024, verbose=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["14997/14997 [==============================] - 13s 881us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tyheGu3rnztE","colab_type":"code","colab":{}},"source":["\n","\n","with open(\"test-unlabelled_evidence_0.8_siameseLSTM_epoch9_with_sentence.json\",'r') as f:\n","      file_content=json.load(f)\n","      for i,key in enumerate(file_content.keys()):\n","          detail_content = file_content[key]\n","          result=eval_predict[i]\n","          lab=np.argmax(result)\n","          if(lab==1):\n","              detail_content['label']='SUPPORTS'\n","          elif(lab==0):\n","              detail_content['label']='REFUTES'\n","          else:\n","              detail_content['label']='NOT ENOUGH INFO'\n","\n","          evidences=detail_content['evidence']\n","          if(evidences==[]):\n","            detail_content['label']='NOT ENOUGH INFO'\n","          for evidence in evidences:\n","            del evidence[2]\n","\n","with open('word2vec-result2.json','w') as t:\n","      json.dump(file_content, t, indent=2, separators=(',',':'))"],"execution_count":0,"outputs":[]}]}